{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py, torch, torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_LATENT_K = 20                 # Number of codebook entries\n",
    "# NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "# BETA = 1.0                        # Weight for the commitment loss\n",
    "\n",
    "# INPUT_SHAPE = x_train.shape[1:]\n",
    "# SIZE = None                       # Spatial size of latent embedding\n",
    "#                                   # will be set dynamically in `build_vqvae\n",
    "\n",
    "# VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
    "# VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "# VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
    "# VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
    "\n",
    "# PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "# PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
    "# PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "# PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "# PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, data_path, indices, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        assert os.path.exists(self.data_path), \"images path {} does not exist\".format(self.data_path)\n",
    "        self.data = h5py.File(self.data_path, 'r')\n",
    "        self.images = self.data['images'][indices]  # array shape [480000,64,64,3], uint8 in range(256)\n",
    "        self.labels = self.data['labels'][indices]  # array shape [480000,6], float64\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Normalize the image to range [-1, 1]\n",
    "        image = 2*(image.astype(np.float32)/255.0) - 1\n",
    "        # Transpose image to fit PyTorch's [C, H, W] format\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #super(VQVAE, self) is a python2 style for running super and call __init__() method of the parent class. In python3 we can use this new typping style.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 24, 4, stride=2, padding=1), # inp_channels=1, out_channels=16, kernel_size (size of receptive field)=4 => width & height output = 14. We have also 16 Weight matrices that result in having an output of shape [14,14,16]\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 8, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.pre_quant_conv = nn.Conv2d(8, 4, kernel_size=1) # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.embedding = nn.Embedding(num_embeddings=5, embedding_dim=4) # Three codebook vectors of size two - For more info regarding Embedding documentation: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.post_quant_conv = nn.Conv2d(4, 24, kernel_size=1)\n",
    "        \n",
    "        # Commitment Loss Beta\n",
    "        self.beta = 0.2\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(24, 6, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(6, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W\n",
    "        encoded_output = self.encoder(x)\n",
    "        quant_input = self.pre_quant_conv(encoded_output)\n",
    "        \n",
    "        ## Quantization\n",
    "        B, C, H, W = quant_input.shape\n",
    "        quant_input = quant_input.permute(0, 2, 3, 1)\n",
    "        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1)))\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))\n",
    "        \n",
    "        # Find index of nearest embedding\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        # Select the embedding weights\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
    "        \n",
    "        # Compute losses\n",
    "        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2) # commitment loss pushes z_x toward embedding space and updates encoded space. It ensures that it does not grow faster than embedding space\n",
    "        codebook_loss = torch.mean((quant_out - quant_input.detach())**2) # codebook loss pushes e_i toward z_x and update embedding space\n",
    "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
    "        \n",
    "        # Ensure straight through gradient\n",
    "        quant_out = quant_input + (quant_out - quant_input).detach()\n",
    "        \n",
    "        # Reshaping back to original input shape\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
    "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
    "        \n",
    "        \n",
    "        ## Decoder part\n",
    "        decoder_input = self.post_quant_conv(quant_out)\n",
    "        output = self.decoder(decoder_input)\n",
    "        return output, quantize_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample_data(n_samples=480_000, seed=42, n_sub_samples=None):\n",
    "    np.random.seed(seed)\n",
    "    if n_sub_samples:\n",
    "        # Generate a list of n_sub_samples random integers between 0 and n_samples-1\n",
    "        indices = np.random.randint(0, n_samples, size=n_sub_samples)\n",
    "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
    "        n_test = int(len(indices)*0.2)\n",
    "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
    "        # Calculate the train indices (remaining 80% of the list)\n",
    "        train_indices = [i for i in indices if i not in test_indices]\n",
    "    else:\n",
    "        # Generate an array between 0 and n_samples-1\n",
    "        indices = np.arange(n_samples)\n",
    "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
    "        n_test = int(len(indices)*0.2)\n",
    "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
    "        # Calculate the train indices (remaining 80% of the list)\n",
    "        train_indices = [i for i in indices if i not in test_indices]\n",
    "    train_indices.sort()\n",
    "    test_indices = test_indices.tolist()\n",
    "    test_indices.sort()\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(train_indices, test_indices, path='/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5'):\n",
    "    train = LoadData(data_path=path, indices=train_indices)\n",
    "    test = LoadData(data_path=path, indices=test_indices)\n",
    "    data_loader = DataLoader(train, batch_size=64, shuffle=True, num_workers=4)\n",
    "    \n",
    "    model = VQVAE().to(device)\n",
    "    \n",
    "    num_epochs = 20\n",
    "    optimizer = Adam(model.parameters(), lr=1E-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for im, label in data_loader:\n",
    "            im = im.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, quantize_loss = model(im)\n",
    "            recon_loss = criterion(out, im)\n",
    "            loss = recon_loss + quantize_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Recon Loss: {recon_loss.item()}, Quantize Loss: {quantize_loss.item()}, Total Loss: {loss.item()}')\n",
    "        print('Finished epoch {}'.format(epoch_idx+1))\n",
    "    print('Done Training...')\n",
    "    \n",
    "    # Reconstruction part\n",
    "    \n",
    "    idxs = torch.randint(0, len(test), (100, ))\n",
    "    ims = torch.cat([test[idx][0][None, :] for idx in idxs]).float()\n",
    "    ims = ims.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    generated_im, _ = model(ims)\n",
    "    ims = (ims+1)/2\n",
    "    generated_im = 1 - (generated_im+1)/2\n",
    "    out = torch.hstack([ims, generated_im])\n",
    "    output = rearrange(out, 'b c h w -> b () h (c w)')\n",
    "    grid = torchvision.utils.make_grid(output.detach().cpu(), nrow=10)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    img.save('reconstruction.png')\n",
    "    # im.show()\n",
    "    \n",
    "    print('Done Reconstruction ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recon Loss: 0.667660653591156, Quantize Loss: 0.25083455443382263, Total Loss: 0.9184951782226562\n",
      "Finished epoch 1\n",
      "Recon Loss: 0.5990063548088074, Quantize Loss: 0.19450423121452332, Total Loss: 0.7935105562210083\n",
      "Finished epoch 2\n",
      "Recon Loss: 0.5863282680511475, Quantize Loss: 0.17511802911758423, Total Loss: 0.7614462971687317\n",
      "Finished epoch 3\n",
      "Recon Loss: 0.5011268854141235, Quantize Loss: 0.17984043061733246, Total Loss: 0.6809673309326172\n",
      "Finished epoch 4\n",
      "Recon Loss: 0.5054752230644226, Quantize Loss: 0.18088242411613464, Total Loss: 0.6863576173782349\n",
      "Finished epoch 5\n",
      "Recon Loss: 0.4698594808578491, Quantize Loss: 0.1983441710472107, Total Loss: 0.6682036519050598\n",
      "Finished epoch 6\n",
      "Recon Loss: 0.44251102209091187, Quantize Loss: 0.19866977632045746, Total Loss: 0.6411808133125305\n",
      "Finished epoch 7\n",
      "Recon Loss: 0.43299946188926697, Quantize Loss: 0.21844150125980377, Total Loss: 0.6514409780502319\n",
      "Finished epoch 8\n",
      "Recon Loss: 0.40997061133384705, Quantize Loss: 0.23678648471832275, Total Loss: 0.6467571258544922\n",
      "Finished epoch 9\n",
      "Recon Loss: 0.38286805152893066, Quantize Loss: 0.25090694427490234, Total Loss: 0.633774995803833\n",
      "Finished epoch 10\n",
      "Recon Loss: 0.41463983058929443, Quantize Loss: 0.2929823696613312, Total Loss: 0.7076221704483032\n",
      "Finished epoch 11\n",
      "Recon Loss: 0.38500308990478516, Quantize Loss: 0.2841695249080658, Total Loss: 0.6691726446151733\n",
      "Finished epoch 12\n",
      "Recon Loss: 0.3804193437099457, Quantize Loss: 0.2796033024787903, Total Loss: 0.6600226163864136\n",
      "Finished epoch 13\n",
      "Recon Loss: 0.4168517589569092, Quantize Loss: 0.2595035433769226, Total Loss: 0.6763553023338318\n",
      "Finished epoch 14\n",
      "Recon Loss: 0.35905230045318604, Quantize Loss: 0.2905258238315582, Total Loss: 0.6495780944824219\n",
      "Finished epoch 15\n",
      "Recon Loss: 0.38694271445274353, Quantize Loss: 0.31618815660476685, Total Loss: 0.703130841255188\n",
      "Finished epoch 16\n",
      "Recon Loss: 0.3564722239971161, Quantize Loss: 0.33059635758399963, Total Loss: 0.6870685815811157\n",
      "Finished epoch 17\n",
      "Recon Loss: 0.34461650252342224, Quantize Loss: 0.34052252769470215, Total Loss: 0.6851390600204468\n",
      "Finished epoch 18\n",
      "Recon Loss: 0.3651401996612549, Quantize Loss: 0.3452107906341553, Total Loss: 0.7103509902954102\n",
      "Finished epoch 19\n",
      "Recon Loss: 0.35642147064208984, Quantize Loss: 0.40388038754463196, Total Loss: 0.7603018283843994\n",
      "Finished epoch 20\n",
      "Done Training...\n",
      "Done Reconstruction ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_indices, test_indices = split_sample_data(n_samples=480_000, seed=42, n_sub_samples=1000)\n",
    "train_vqvae(train_indices, test_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
