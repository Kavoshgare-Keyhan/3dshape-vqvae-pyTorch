{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ZahraFayyaz/3dshape-vqvae-pyTorch/blob/main/3dshape_vqvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Uqt-IlG47el4"},"source":["# Import modules"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Cmxk1jIK7el6","executionInfo":{"status":"ok","timestamp":1719827908266,"user_tz":-120,"elapsed":13434,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["import os, h5py, torch, torchvision, timeit\n","import torch.nn as nn\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam"]},{"cell_type":"markdown","metadata":{"id":"DL_a88gxCCdZ"},"source":["## Modify PyTorch Configuration"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"tNH6Uzj2CCda","executionInfo":{"status":"ok","timestamp":1719827910855,"user_tz":-120,"elapsed":256,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["# Enable CuDNN benchmark mode for faster runtime optimizations\n","torch.backends.cudnn.benchmark = True\n","\n","# Ensure deterministic behavior in CuDNN operations\n","torch.backends.cudnn.deterministic = True\n","\n","# Optionally disable CuDNN's tensor core usage if encountering issues\n","torch.backends.cudnn.enabled = True"]},{"cell_type":"markdown","metadata":{"id":"zXKxalFs7el7"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNAguA087el7"},"outputs":[],"source":["# NUM_LATENT_K = 20                 # Number of codebook entries\n","# NUM_LATENT_D = 64                 # Dimension of each codebook entries\n","# BETA = 1.0                        # Weight for the commitment loss\n","\n","# INPUT_SHAPE = x_train.shape[1:]\n","# SIZE = None                       # Spatial size of latent embedding\n","#                                   # will be set dynamically in `build_vqvae\n","\n","# VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n","# VQVAE_NUM_EPOCHS = 20             # Number of epochs\n","# VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n","# VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n","\n","# PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n","# PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n","# PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n","# PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n","# PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block"]},{"cell_type":"markdown","metadata":{"id":"movEMY4M7el8"},"source":["# Load Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGtE-4-7CSC9","outputId":"32b312e5-3d44-4132-b149-9297c409975a","executionInfo":{"status":"ok","timestamp":1719827934433,"user_tz":-120,"elapsed":20135,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#connect to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psYAsanlTXIJ"},"outputs":[],"source":["class LoadData_fast(Dataset):\n","    def __init__(self, data_path, indices, transform=None):\n","        self.data_path = data_path\n","        self.transform = transform\n","        assert os.path.exists(self.data_path), f\"images path {self.data_path} does not exist\"\n","        self.data = h5py.File(self.data_path, 'r')\n","        self.indices = indices  # Store indices for lazy loading\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitems__(self, index):\n","        idx = self.indices[index]\n","        image = self.data['images'][idx]  # Load image on-demand\n","        label = self.data['labels'][idx]  # Load label on-demand\n","\n","        # Normalize the image to range [-1, 1] and transpose to [C, H, W] format\n","        image = 2 * (image.astype(np.float32) / 255.0) - 1\n","        image = np.transpose(image, (2, 0, 1))\n","\n","        # Convert to torch tensors\n","        image = torch.tensor(image, dtype=torch.float32)\n","        label = torch.tensor(label, dtype=torch.float32)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CmecR3M4CCdf","executionInfo":{"status":"ok","timestamp":1719827950182,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["class Shapes3DDataset(Dataset):\n","    def __init__(self, split, data_path, transform=None):\n","        self.data_path = data_path\n","        assert os.path.exists(self.data_path), f\"images path {self.data_path} does not exist\"\n","        self.dataset = h5py.File(self.data_path, 'r')\n","        if split=='train': self.images = self.dataset['images'][:384_000,:]\n","        elif split=='test': self.images = self.dataset['images'][384_000:,:]\n","        # self.labels = self.file['labels']\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.images.shape[0]\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        # label = self.labels[idx]\n","\n","        # Normalize the image to range [-1, 1]\n","        image = image.astype(np.float32) / 255.0 * 2 - 1\n","        # Transpose image to fit PyTorch's [C, H, W] format\n","        image = np.transpose(image, (2, 0, 1))\n","\n","        image = torch.tensor(image, dtype=torch.float32)\n","        # label = torch.tensor(label, dtype=torch.float32)\n","\n","        return image, None\n","\n","# Custom collate function to handle batch loading\n","def custom_collate_fn(batch):\n","    images = torch.stack([item[0] for item in batch])\n","    # labels = torch.stack([item[1] for item in batch])\n","    return images, None\n","\n","\n","# Initialize the dataset and dataloader\n","# dataset = Shapes3DDataset(data_path='/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5')\n","# dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"bS-M9DNlCCdg"},"source":["## Sample & Split Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAqMPxLDCCdg"},"outputs":[],"source":["#make the split_sample_data function faster\n","def split_sample_data_fast(n_samples=480_000, seed=42, n_sub_samples=None):\n","    np.random.seed(seed)\n","\n","    if n_sub_samples:\n","        # Generate a list of n_sub_samples random integers between 0 and n_samples-1\n","        indices = np.random.randint(0, n_samples, size=n_sub_samples)\n","    else:\n","        # Generate an array between 0 and n_samples-1\n","        indices = np.arange(n_samples)\n","\n","    # Calculate the number of test indices (20% of the list) and extract the test indices\n","    n_test = int(len(indices) * 0.2)\n","    test_indices = np.random.choice(indices, size=n_test, replace=False)\n","\n","    # Create a boolean mask to identify train indices\n","    mask = np.isin(indices, test_indices, invert=True)\n","    train_indices = indices[mask]\n","\n","    # Sort indices if required (optional)\n","    train_indices.sort()\n","    test_indices.sort()\n","\n","    return train_indices.tolist(), test_indices.tolist()"]},{"cell_type":"markdown","metadata":{"id":"3JwVBiVs7el8"},"source":["# VQVAE Class"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"AdYwfyO17el9","executionInfo":{"status":"ok","timestamp":1719827992042,"user_tz":-120,"elapsed":267,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["class VQVAE(nn.Module):\n","    def __init__(self):\n","        super().__init__() #super(VQVAE, self) is a python2 style for running super and call __init__() method of the parent class. In python3 we can use this new typping style.\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 24, 4, stride=2, padding=1), # inp_channels=1, out_channels=16, kernel_size (size of receptive field)=4 => width & height output = 14. We have also 16 Weight matrices that result in having an output of shape [14,14,16]\n","            nn.BatchNorm2d(24),\n","            nn.ReLU(),\n","            nn.Conv2d(24, 48, 4, stride=2, padding=1),\n","            nn.BatchNorm2d(48),\n","            nn.ReLU(),\n","        )\n","\n","        self.pre_quant_conv = nn.Conv2d(48, 64, kernel_size=1) # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n","        self.embedding = nn.Embedding(num_embeddings=20, embedding_dim=64) # Three codebook vectors of size two - For more info regarding Embedding documentation: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n","        self.post_quant_conv = nn.Conv2d(64, 48, kernel_size=1)\n","\n","        # Commitment Loss Beta\n","        self.beta = 0.2\n","\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),\n","            nn.BatchNorm2d(24),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(24, 3, 4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","\n","    def forward(self, x):\n","        # B, C, H, W\n","        encoded_output = self.encoder(x)\n","        quant_input = self.pre_quant_conv(encoded_output)\n","\n","        ## Quantization\n","        B, C, H, W = quant_input.shape # stands for batch_size, output channel of convolution before quantization, height, and width\n","        quant_input = quant_input.permute(0, 2, 3, 1) # replace position of B, C, H, W to B, H, W, C\n","        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1))) # flatten the data shape to B, W*H, C\n","\n","        # Compute pairwise distances\n","        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))\n","\n","        # Find index of nearest embedding\n","        min_encoding_indices = torch.argmin(dist, dim=-1)\n","\n","        # Select the embedding weights\n","        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1)) # index_select selects the nearest embedding weights based on min_encoding_indices.\n","        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n","\n","        # Compute losses\n","        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2) # commitment loss pushes z_x toward embedding space and updates encoded space. It ensures that it does not grow faster than embedding space\n","        codebook_loss = torch.mean((quant_out - quant_input.detach())**2) # codebook loss pushes e_i toward z_x and update embedding space\n","        quantize_losses = codebook_loss + self.beta*commitment_loss\n","\n","        # Ensure straight through gradient\n","        quant_out = quant_input + (quant_out - quant_input).detach()\n","\n","        # Reshaping back to original input shape\n","        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2) # Changes the shpae of quant_input from [B, H, W, C] to [B, C, H, W]\n","        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n","\n","\n","        ## Decoder part\n","        decoder_input = self.post_quant_conv(quant_out)\n","        output = self.decoder(decoder_input)\n","        return output, quantize_losses"]},{"cell_type":"markdown","metadata":{"id":"-AhokjOX7el-"},"source":["# VQVAE Implementation"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"vVEH1psC7el-","executionInfo":{"status":"ok","timestamp":1719827995378,"user_tz":-120,"elapsed":266,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["def train_vqvae(path='/content/drive/MyDrive/Data/3dshapes.h5', device='cuda'):\n","    train_dataset = Shapes3DDataset(split='train', data_path=path)\n","    test_dataset = Shapes3DDataset(split='test', data_path=path)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn)\n","    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn)\n","\n","    # Initialize model, optimizer, and loss function\n","    model = VQVAE().to(device)\n","    optimizer = Adam(model.parameters(), lr=1E-3)\n","    criterion = torch.nn.MSELoss()\n","\n","    num_epochs = 20\n","\n","    for epoch_idx in range(num_epochs):\n","        model.train()  # Set model to training mode\n","        for im, label in train_loader:\n","            im = im.float().to(device, non_blocking=True)\n","            optimizer.zero_grad()\n","            out, quantize_loss = model(im)\n","            recon_loss = criterion(out, im)\n","            loss = recon_loss + quantize_loss\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch {epoch_idx+1}: Recon Loss: {recon_loss.item()}, Quantize Loss: {quantize_loss.item()}, Total Loss: {loss.item()}')\n","    print('Done Training...')\n","\n","    model.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        total_recon_loss = 0\n","        total_quantize_loss = 0\n","        for im, label in test_loader:\n","            im = im.float().to(device, non_blocking=True)\n","            out, quantize_loss = model(im)\n","            recon_loss = criterion(out, im)\n","            total_recon_loss += recon_loss.item()\n","            total_quantize_loss += quantize_loss.item()\n","\n","        print(f'Validation: Recon Loss: {total_recon_loss/len(test_loader)}, Quantize Loss: {total_quantize_loss/len(test_loader)}')\n","\n","    idxs = torch.randint(0, len(test_dataset), (100, )) # Randomly sample indices for reconstruction.\n","    ims = torch.cat([test_dataset[idx][0][None, :] for idx in idxs]).float() # Create a batch of images from the test set using sampled indices.\n","    ims = ims.to(device)\n","    model.eval()\n","\n","\n","    generated_im, _ = model(ims) # Generate reconstructed images.\n","    ims = (ims+1)/2\n","    generated_im = (generated_im + 1) / 2  # Normalize to [0, 1] for visualization\n","\n","    ## Transform to original value to retrieve colorized data\n","    ims = ims * 255.0  # Scale to [0, 255]\n","    generated_im = generated_im * 255.0  # Scale to [0, 255]\n","    ims = ims.cpu().numpy().astype(np.uint8)\n","    generated_im = generated_im.detach().cpu().numpy().astype(np.uint8)  # Detach before converting to NumPy\n","\n","    # combined_images = torch.hstack([ims, generated_im])\n","    combined_images = np.concatenate((ims, generated_im), axis=3)\n","\n","    # Rearrange to a grid\n","    combined_images = torch.tensor(combined_images).permute(0, 1, 2, 3)  # Change to [batch, channels, height, width]\n","\n","    grid = torchvision.utils.make_grid(combined_images, nrow=10, padding=2)\n","\n","    img = torchvision.transforms.ToPILImage()(grid)\n","    img.save('/content/drive/My Drive/INI - Generative Episodic Memory/reconstruction.png')\n","    # im.show()\n","\n","    print('Done Reconstruction ...')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UO7J5CoSUl2h"},"outputs":[],"source":["\n","def train_vqvae_fast(train_indices, test_indices, path='/content/drive/MyDrive/Data/3dshapes.h5', device='cuda'):\n","    # Load datasets\n","    train_dataset = LoadData_fast(data_path=path, indices=train_indices)\n","    test_dataset = LoadData_fast(data_path=path, indices=test_indices)\n","\n","    # Create data loaders\n","    data_loader = DataLoader(train_dataset, num_workers=4, pin_memory=True) # , batch_size=256\n","    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=8, pin_memory=True)\n","\n","    # Initialize model, optimizer, and loss function\n","    model = VQVAE().to(device)\n","    optimizer = Adam(model.parameters(), lr=1E-3)\n","    criterion = torch.nn.MSELoss()\n","\n","    num_epochs = 5\n","\n","    for epoch_idx in range(num_epochs):\n","        model.train()  # Set model to training mode\n","        for im, label in data_loader:\n","            im = im.float().to(device, non_blocking=True)\n","\n","            optimizer.zero_grad()\n","            out, quantize_loss = model(im)\n","            recon_loss = criterion(out, im)\n","            loss = recon_loss + quantize_loss\n","            loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch {epoch_idx+1}: Recon Loss: {recon_loss.item()}, Quantize Loss: {quantize_loss.item()}, Total Loss: {loss.item()}')\n","\n","        # model.eval()  # Set model to evaluation mode\n","        # with torch.no_grad():\n","        #     total_recon_loss = 0\n","        #     total_quantize_loss = 0\n","        #     for im, label in test_loader:\n","        #         im = im.float().to(device, non_blocking=True)\n","        #         out, quantize_loss = model(im)\n","        #         recon_loss = criterion(out, im)\n","        #         total_recon_loss += recon_loss.item()\n","        #         total_quantize_loss += quantize_loss.item()\n","\n","        # print(f'Validation: Recon Loss: {total_recon_loss/len(test_loader)}, Quantize Loss: {total_quantize_loss/len(test_loader)}')\n","\n","    print('Done Training...')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uakGThWv7el-","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719829225676,"user_tz":-120,"elapsed":1225806,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}},"outputId":"e16464fe-c626-43f7-d91a-9996ac0e2d15"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Recon Loss: 0.09123313426971436, Quantize Loss: 0.5732313990592957, Total Loss: 0.66446453332901\n","Epoch 2: Recon Loss: 0.06654424965381622, Quantize Loss: 0.29619789123535156, Total Loss: 0.3627421259880066\n","Epoch 3: Recon Loss: 0.0519179068505764, Quantize Loss: 0.29390987753868103, Total Loss: 0.34582778811454773\n","Epoch 4: Recon Loss: 0.04746844619512558, Quantize Loss: 0.2970633804798126, Total Loss: 0.3445318341255188\n","Epoch 5: Recon Loss: 0.03932037577033043, Quantize Loss: 0.19757023453712463, Total Loss: 0.23689061403274536\n","Epoch 6: Recon Loss: 0.03514795005321503, Quantize Loss: 0.22664684057235718, Total Loss: 0.2617948055267334\n","Epoch 7: Recon Loss: 0.03491806983947754, Quantize Loss: 0.2194506824016571, Total Loss: 0.25436875224113464\n","Epoch 8: Recon Loss: 0.036480605602264404, Quantize Loss: 0.22345922887325287, Total Loss: 0.25993984937667847\n","Epoch 9: Recon Loss: 0.037030309438705444, Quantize Loss: 0.24007457494735718, Total Loss: 0.2771048843860626\n","Epoch 10: Recon Loss: 0.034072719514369965, Quantize Loss: 0.26418405771255493, Total Loss: 0.2982567846775055\n","Epoch 11: Recon Loss: 0.0344054251909256, Quantize Loss: 0.2602587342262268, Total Loss: 0.2946641445159912\n","Epoch 12: Recon Loss: 0.033257853239774704, Quantize Loss: 0.22813186049461365, Total Loss: 0.26138970255851746\n","Epoch 13: Recon Loss: 0.033211857080459595, Quantize Loss: 0.2703224718570709, Total Loss: 0.3035343289375305\n","Epoch 14: Recon Loss: 0.03135373070836067, Quantize Loss: 0.2570304870605469, Total Loss: 0.28838422894477844\n","Epoch 15: Recon Loss: 0.0318189337849617, Quantize Loss: 0.24669486284255981, Total Loss: 0.2785137891769409\n","Epoch 16: Recon Loss: 0.026707500219345093, Quantize Loss: 0.20452101528644562, Total Loss: 0.2312285155057907\n","Epoch 17: Recon Loss: 0.027202516794204712, Quantize Loss: 0.21575134992599487, Total Loss: 0.24295386672019958\n","Epoch 18: Recon Loss: 0.025820698589086533, Quantize Loss: 0.20158651471138, Total Loss: 0.22740721702575684\n","Epoch 19: Recon Loss: 0.02509061060845852, Quantize Loss: 0.19596846401691437, Total Loss: 0.22105906903743744\n","Epoch 20: Recon Loss: 0.026393307372927666, Quantize Loss: 0.1964985430240631, Total Loss: 0.22289185225963593\n","Done Training...\n","Validation: Recon Loss: 0.045995786915222804, Quantize Loss: 0.37853438313802085\n","Done Reconstruction ...\n"]}],"source":["# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# train_indices, test_indices = split_sample_data_fast(n_samples=480_000, seed=42, n_sub_samples=1000)\n","train_vqvae()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}