{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, h5py, torch, torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_LATENT_K = 20                 # Number of codebook entries\n",
    "# NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "# BETA = 1.0                        # Weight for the commitment loss\n",
    "\n",
    "# INPUT_SHAPE = x_train.shape[1:]\n",
    "# SIZE = None                       # Spatial size of latent embedding\n",
    "#                                   # will be set dynamically in `build_vqvae\n",
    "\n",
    "# VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
    "# VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "# VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
    "# VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
    "\n",
    "# PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "# PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
    "# PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "# PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "# PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, data_path, indices, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        assert os.path.exists(self.data_path), \"images path {} does not exist\".format(self.data_path)\n",
    "        self.data = h5py.File(self.data_path, 'r')\n",
    "        self.images = self.data['images'][indices]  # array shape [480000,64,64,3], uint8 in range(256)\n",
    "        self.labels = self.data['labels'][indices]  # array shape [480000,6], float64\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Normalize the image to range [-1, 1]\n",
    "        image = 2*(image.astype(np.float32)/255.0) - 1\n",
    "        # Transpose image to fit PyTorch's [C, H, W] format\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #super(VQVAE, self) is a python2 style for running super and call __init__() method of the parent class. In python3 we can use this new typping style.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 24, 4, stride=2, padding=1), # inp_channels=1, out_channels=16, kernel_size (size of receptive field)=4 => width & height output = 14. We have also 16 Weight matrices that result in having an output of shape [14,14,16]\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 8, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.pre_quant_conv = nn.Conv2d(8, 4, kernel_size=1) # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.embedding = nn.Embedding(num_embeddings=5, embedding_dim=4) # Three codebook vectors of size two - For more info regarding Embedding documentation: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.post_quant_conv = nn.Conv2d(4, 24, kernel_size=1)\n",
    "        \n",
    "        # Commitment Loss Beta\n",
    "        self.beta = 0.2\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(24, 6, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(6, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W\n",
    "        encoded_output = self.encoder(x)\n",
    "        quant_input = self.pre_quant_conv(encoded_output)\n",
    "        \n",
    "        ## Quantization\n",
    "        B, C, H, W = quant_input.shape # stands for batch_size, output channel of convolution before quantization, height, and width\n",
    "        quant_input = quant_input.permute(0, 2, 3, 1) # replace position of B, C, H, W to B, H, W, C\n",
    "        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1))) # flatten the data shape to B, W*H, C\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))\n",
    "        \n",
    "        # Find index of nearest embedding\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        # Select the embedding weights\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1)) # index_select selects the nearest embedding weights based on min_encoding_indices.\n",
    "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
    "        \n",
    "        # Compute losses\n",
    "        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2) # commitment loss pushes z_x toward embedding space and updates encoded space. It ensures that it does not grow faster than embedding space\n",
    "        codebook_loss = torch.mean((quant_out - quant_input.detach())**2) # codebook loss pushes e_i toward z_x and update embedding space\n",
    "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
    "        \n",
    "        # Ensure straight through gradient\n",
    "        quant_out = quant_input + (quant_out - quant_input).detach()\n",
    "        \n",
    "        # Reshaping back to original input shape\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2) # Changes the shpae of quant_input from [B, H, W, C] to [B, C, H, W]\n",
    "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
    "        \n",
    "        \n",
    "        ## Decoder part\n",
    "        decoder_input = self.post_quant_conv(quant_out)\n",
    "        output = self.decoder(decoder_input)\n",
    "        return output, quantize_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample_data(n_samples=480_000, seed=42, n_sub_samples=None):\n",
    "    np.random.seed(seed)\n",
    "    if n_sub_samples:\n",
    "        # Generate a list of n_sub_samples random integers between 0 and n_samples-1\n",
    "        indices = np.random.randint(0, n_samples, size=n_sub_samples)\n",
    "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
    "        n_test = int(len(indices)*0.2)\n",
    "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
    "        # Calculate the train indices (remaining 80% of the list)\n",
    "        train_indices = [i for i in indices if i not in test_indices]\n",
    "    else:\n",
    "        # Generate an array between 0 and n_samples-1\n",
    "        indices = np.arange(n_samples)\n",
    "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
    "        n_test = int(len(indices)*0.2)\n",
    "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
    "        # Calculate the train indices (remaining 80% of the list)\n",
    "        train_indices = [i for i in indices if i not in test_indices]\n",
    "    train_indices.sort()\n",
    "    test_indices = test_indices.tolist()\n",
    "    test_indices.sort()\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(train_indices, test_indices, path='/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5'):\n",
    "    train = LoadData(data_path=path, indices=train_indices)\n",
    "    test = LoadData(data_path=path, indices=test_indices)\n",
    "    data_loader = DataLoader(train, batch_size=64, shuffle=True, num_workers=4)\n",
    "    \n",
    "    model = VQVAE().to(device)\n",
    "    \n",
    "    num_epochs = 20\n",
    "    optimizer = Adam(model.parameters(), lr=1E-3) # Adam optimizer for gradient descent.\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for im, label in data_loader:\n",
    "            im = im.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, quantize_loss = model(im)\n",
    "            recon_loss = criterion(out, im)\n",
    "            loss = recon_loss + quantize_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Recon Loss: {recon_loss.item()}, Quantize Loss: {quantize_loss.item()}, Total Loss: {loss.item()}')\n",
    "        print('Finished epoch {}'.format(epoch_idx+1))\n",
    "    print('Done Training...')\n",
    "    \n",
    "    # Reconstruction part\n",
    "    \n",
    "    idxs = torch.randint(0, len(test), (100, )) # Randomly sample indices for reconstruction.\n",
    "    ims = torch.cat([test[idx][0][None, :] for idx in idxs]).float() # Create a batch of images from the test set using sampled indices.\n",
    "    ims = ims.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    generated_im, _ = model(ims) # Generate reconstructed images.\n",
    "    ims = (ims+1)/2\n",
    "    generated_im = (generated_im + 1) / 2  # Normalize to [0, 1] for visualization\n",
    "\n",
    "    ## Transform to original value to retrieve colorized data\n",
    "    ims = ims * 255.0  # Scale to [0, 255]\n",
    "    generated_im = generated_im * 255.0  # Scale to [0, 255]\n",
    "    ims = ims.cpu().numpy().astype(np.uint8)\n",
    "    generated_im = generated_im.detach().cpu().numpy().astype(np.uint8)  # Detach before converting to NumPy\n",
    "\n",
    "    # combined_images = torch.hstack([ims, generated_im])\n",
    "    combined_images = np.concatenate((ims, generated_im), axis=3) \n",
    "\n",
    "    # Rearrange to a grid\n",
    "    combined_images = torch.tensor(combined_images).permute(0, 1, 2, 3)  # Change to [batch, channels, height, width]\n",
    "    print(combined_images.shape)\n",
    "\n",
    "    grid = torchvision.utils.make_grid(combined_images, nrow=10, padding=2)\n",
    "    # grid = torchvision.utils.make_grid(combined_images.detach().cpu(), nrow=10)\n",
    "    \n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    img.save('reconstruction.png')\n",
    "    # im.show()\n",
    "    \n",
    "    print('Done Reconstruction ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recon Loss: 0.6700550317764282, Quantize Loss: 0.4744310677051544, Total Loss: 1.1444860696792603\n",
      "Finished epoch 1\n",
      "Recon Loss: 0.6213817596435547, Quantize Loss: 0.41797935962677, Total Loss: 1.0393611192703247\n",
      "Finished epoch 2\n",
      "Recon Loss: 0.6019285917282104, Quantize Loss: 0.37301716208457947, Total Loss: 0.9749457836151123\n",
      "Finished epoch 3\n",
      "Recon Loss: 0.5729265213012695, Quantize Loss: 0.34419357776641846, Total Loss: 0.917120099067688\n",
      "Finished epoch 4\n",
      "Recon Loss: 0.5550350546836853, Quantize Loss: 0.326232373714447, Total Loss: 0.8812674283981323\n",
      "Finished epoch 5\n",
      "Recon Loss: 0.5243699550628662, Quantize Loss: 0.3162420392036438, Total Loss: 0.84061199426651\n",
      "Finished epoch 6\n",
      "Recon Loss: 0.5079967975616455, Quantize Loss: 0.29876089096069336, Total Loss: 0.8067576885223389\n",
      "Finished epoch 7\n",
      "Recon Loss: 0.5034692287445068, Quantize Loss: 0.2779771089553833, Total Loss: 0.7814463376998901\n",
      "Finished epoch 8\n",
      "Recon Loss: 0.47275376319885254, Quantize Loss: 0.25334811210632324, Total Loss: 0.7261018753051758\n",
      "Finished epoch 9\n",
      "Recon Loss: 0.4538748264312744, Quantize Loss: 0.23590128123760223, Total Loss: 0.6897761225700378\n",
      "Finished epoch 10\n",
      "Recon Loss: 0.46046149730682373, Quantize Loss: 0.23088660836219788, Total Loss: 0.6913480758666992\n",
      "Finished epoch 11\n",
      "Recon Loss: 0.4440907835960388, Quantize Loss: 0.21888293325901031, Total Loss: 0.6629737019538879\n",
      "Finished epoch 12\n",
      "Recon Loss: 0.4376370906829834, Quantize Loss: 0.21231794357299805, Total Loss: 0.6499550342559814\n",
      "Finished epoch 13\n",
      "Recon Loss: 0.41390395164489746, Quantize Loss: 0.2191629707813263, Total Loss: 0.6330668926239014\n",
      "Finished epoch 14\n",
      "Recon Loss: 0.4142569899559021, Quantize Loss: 0.2311101257801056, Total Loss: 0.6453671455383301\n",
      "Finished epoch 15\n",
      "Recon Loss: 0.42225557565689087, Quantize Loss: 0.2273014485836029, Total Loss: 0.6495569944381714\n",
      "Finished epoch 16\n",
      "Recon Loss: 0.38957929611206055, Quantize Loss: 0.23975875973701477, Total Loss: 0.6293380260467529\n",
      "Finished epoch 17\n",
      "Recon Loss: 0.39279162883758545, Quantize Loss: 0.22378435730934143, Total Loss: 0.6165759563446045\n",
      "Finished epoch 18\n",
      "Recon Loss: 0.376039981842041, Quantize Loss: 0.20807752013206482, Total Loss: 0.5841175317764282\n",
      "Finished epoch 19\n",
      "Recon Loss: 0.3956974446773529, Quantize Loss: 0.21401084959506989, Total Loss: 0.609708309173584\n",
      "Finished epoch 20\n",
      "Done Training...\n",
      "(100, 3, 64, 128)\n",
      "torch.Size([100, 3, 128, 64])\n",
      "Done Reconstruction ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_indices, test_indices = split_sample_data(n_samples=480_000, seed=42, n_sub_samples=320)\n",
    "train_vqvae(train_indices, test_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
