{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraFayyaz/3dshape-vqvae-pyTorch/blob/main/3dshape_vqvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqt-IlG47el4"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "id": "UTJrqQYB7uax",
        "outputId": "f13d60f7-8425-4d71-9992-8e7804001e12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cmxk1jIK7el6"
      },
      "outputs": [],
      "source": [
        "import os, h5py, torch, torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKxalFs7el7"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PNAguA087el7"
      },
      "outputs": [],
      "source": [
        "# NUM_LATENT_K = 20                 # Number of codebook entries\n",
        "# NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
        "# BETA = 1.0                        # Weight for the commitment loss\n",
        "\n",
        "# INPUT_SHAPE = x_train.shape[1:]\n",
        "# SIZE = None                       # Spatial size of latent embedding\n",
        "#                                   # will be set dynamically in `build_vqvae\n",
        "\n",
        "# VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
        "# VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
        "# VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
        "# VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
        "\n",
        "# PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
        "# PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
        "# PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
        "# PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
        "# PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "movEMY4M7el8"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bGtE-4-7CSC9",
        "outputId": "fc8779fb-6ade-4506-a61f-68cb517be8a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vkViRQgk7el8"
      },
      "outputs": [],
      "source": [
        "class LoadData(Dataset):\n",
        "    def __init__(self, data_path, indices, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        assert os.path.exists(self.data_path), \"images path {} does not exist\".format(self.data_path)\n",
        "        self.data = h5py.File(self.data_path, 'r')\n",
        "        self.images = self.data['images'][indices]  # array shape [480000,64,64,3], uint8 in range(256)\n",
        "        self.labels = self.data['labels'][indices]  # array shape [480000,6], float64\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        # Normalize the image to range [-1, 1]\n",
        "        image = 2*(image.astype(np.float32)/255.0) - 1\n",
        "        # Transpose image to fit PyTorch's [C, H, W] format\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LoadData_fast(Dataset):\n",
        "    def __init__(self, data_path, indices, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        assert os.path.exists(self.data_path), f\"images path {self.data_path} does not exist\"\n",
        "        self.data = h5py.File(self.data_path, 'r')\n",
        "        self.indices = indices  # Store indices for lazy loading\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = self.indices[index]\n",
        "        image = self.data['images'][idx]  # Load image on-demand\n",
        "        label = self.data['labels'][idx]  # Load label on-demand\n",
        "\n",
        "        # Normalize the image to range [-1, 1] and transpose to [C, H, W] format\n",
        "        image = 2 * (image.astype(np.float32) / 255.0) - 1\n",
        "        image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "psYAsanlTXIJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JwVBiVs7el8"
      },
      "source": [
        "# VQVAE Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AdYwfyO17el9"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__() #super(VQVAE, self) is a python2 style for running super and call __init__() method of the parent class. In python3 we can use this new typping style.\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 24, 4, stride=2, padding=1), # inp_channels=1, out_channels=16, kernel_size (size of receptive field)=4 => width & height output = 14. We have also 16 Weight matrices that result in having an output of shape [14,14,16]\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(24, 8, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.pre_quant_conv = nn.Conv2d(8, 4, kernel_size=1) # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "        self.embedding = nn.Embedding(num_embeddings=5, embedding_dim=4) # Three codebook vectors of size two - For more info regarding Embedding documentation: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "        self.post_quant_conv = nn.Conv2d(4, 24, kernel_size=1)\n",
        "\n",
        "        # Commitment Loss Beta\n",
        "        self.beta = 0.2\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(24, 6, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(6, 3, 4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # B, C, H, W\n",
        "        encoded_output = self.encoder(x)\n",
        "        quant_input = self.pre_quant_conv(encoded_output)\n",
        "\n",
        "        ## Quantization\n",
        "        B, C, H, W = quant_input.shape # stands for batch_size, output channel of convolution before quantization, height, and width\n",
        "        quant_input = quant_input.permute(0, 2, 3, 1) # replace position of B, C, H, W to B, H, W, C\n",
        "        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1))) # flatten the data shape to B, W*H, C\n",
        "\n",
        "        # Compute pairwise distances\n",
        "        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))\n",
        "\n",
        "        # Find index of nearest embedding\n",
        "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
        "\n",
        "        # Select the embedding weights\n",
        "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1)) # index_select selects the nearest embedding weights based on min_encoding_indices.\n",
        "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
        "\n",
        "        # Compute losses\n",
        "        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2) # commitment loss pushes z_x toward embedding space and updates encoded space. It ensures that it does not grow faster than embedding space\n",
        "        codebook_loss = torch.mean((quant_out - quant_input.detach())**2) # codebook loss pushes e_i toward z_x and update embedding space\n",
        "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
        "\n",
        "        # Ensure straight through gradient\n",
        "        quant_out = quant_input + (quant_out - quant_input).detach()\n",
        "\n",
        "        # Reshaping back to original input shape\n",
        "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2) # Changes the shpae of quant_input from [B, H, W, C] to [B, C, H, W]\n",
        "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
        "\n",
        "\n",
        "        ## Decoder part\n",
        "        decoder_input = self.post_quant_conv(quant_out)\n",
        "        output = self.decoder(decoder_input)\n",
        "        return output, quantize_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7GxLYJK7el9"
      },
      "source": [
        "# Sample & Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-zVAo7on7el9"
      },
      "outputs": [],
      "source": [
        "def split_sample_data(n_samples=480_000, seed=42, n_sub_samples=None):\n",
        "    np.random.seed(seed)\n",
        "    if n_sub_samples:\n",
        "        # Generate a list of n_sub_samples random integers between 0 and n_samples-1\n",
        "        indices = np.random.randint(0, n_samples, size=n_sub_samples)\n",
        "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
        "        n_test = int(len(indices)*0.2)\n",
        "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
        "        # Calculate the train indices (remaining 80% of the list)\n",
        "        train_indices = [i for i in indices if i not in test_indices]\n",
        "    else:\n",
        "        # Generate an array between 0 and n_samples-1\n",
        "        indices = np.arange(n_samples)\n",
        "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
        "        n_test = int(len(indices)*0.2)\n",
        "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
        "        # Calculate the train indices (remaining 80% of the list)\n",
        "        train_indices = [i for i in indices if i not in test_indices]\n",
        "    train_indices.sort()\n",
        "    test_indices = test_indices.tolist()\n",
        "    test_indices.sort()\n",
        "    return train_indices, test_indices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make the split_sample_data function faster\n",
        "def split_sample_data_fast(n_samples=480_000, seed=42, n_sub_samples=None):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if n_sub_samples:\n",
        "        # Generate a list of n_sub_samples random integers between 0 and n_samples-1\n",
        "        indices = np.random.randint(0, n_samples, size=n_sub_samples)\n",
        "    else:\n",
        "        # Generate an array between 0 and n_samples-1\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "    # Calculate the number of test indices (20% of the list) and extract the test indices\n",
        "    n_test = int(len(indices) * 0.2)\n",
        "    test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
        "\n",
        "    # Create a boolean mask to identify train indices\n",
        "    mask = np.isin(indices, test_indices, invert=True)\n",
        "    train_indices = indices[mask]\n",
        "\n",
        "    # Sort indices if required (optional)\n",
        "    train_indices.sort()\n",
        "    test_indices.sort()\n",
        "\n",
        "    return train_indices.tolist(), test_indices.tolist()"
      ],
      "metadata": {
        "id": "biZcj3e7MvgO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AhokjOX7el-"
      },
      "source": [
        "# VQVAE Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vVEH1psC7el-"
      },
      "outputs": [],
      "source": [
        "def train_vqvae(train_indices, test_indices, path='/content/drive/MyDrive/Data/3dshapes.h5'):\n",
        "    train = LoadData_fast(data_path=path, indices=train_indices)\n",
        "    test = LoadData_fast(data_path=path, indices=test_indices)\n",
        "    data_loader = DataLoader(train, batch_size=64, shuffle=True, num_workers=4)\n",
        "\n",
        "    model = VQVAE().to(device)\n",
        "\n",
        "    num_epochs = 2\n",
        "    optimizer = Adam(model.parameters(), lr=1E-3) # Adam optimizer for gradient descent.\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        for im, label in data_loader:\n",
        "            im = im.float().to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out, quantize_loss = model(im)\n",
        "            recon_loss = criterion(out, im)\n",
        "            loss = recon_loss + quantize_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Recon Loss: {recon_loss.item()}, Quantize Loss: {quantize_loss.item()}, Total Loss: {loss.item()}')\n",
        "        print('Finished epoch {}'.format(epoch_idx+1))\n",
        "    print('Done Training...')\n",
        "\n",
        "    # Reconstruction part\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_vqvae_fast(train_indices, test_indices, path='/content/drive/MyDrive/Data/3dshapes.h5', device='cuda'):\n",
        "    # Load datasets\n",
        "    train_dataset = LoadData_fast(data_path=path, indices=train_indices)\n",
        "    test_dataset = LoadData_fast(data_path=path, indices=test_indices)\n",
        "\n",
        "    # Create data loaders\n",
        "    data_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Initialize model, optimizer, and loss function\n",
        "    model = VQVAE().to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=1E-3)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        for im, label in data_loader:\n",
        "            im = im.float().to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out, quantize_loss = model(im)\n",
        "            recon_loss = criterion(out, im)\n",
        "            loss = recon_loss + quantize_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch_idx+1}: Recon Loss: {recon_loss.item()}, Quantize Loss: {quantize_loss.item()}, Total Loss: {loss.item()}')\n",
        "\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            total_recon_loss = 0\n",
        "            total_quantize_loss = 0\n",
        "            for im, label in test_loader:\n",
        "                im = im.float().to(device, non_blocking=True)\n",
        "                out, quantize_loss = model(im)\n",
        "                recon_loss = criterion(out, im)\n",
        "                total_recon_loss += recon_loss.item()\n",
        "                total_quantize_loss += quantize_loss.item()\n",
        "\n",
        "        print(f'Validation: Recon Loss: {total_recon_loss/len(test_loader)}, Quantize Loss: {total_quantize_loss/len(test_loader)}')\n",
        "\n",
        "    print('Done Training...')"
      ],
      "metadata": {
        "id": "UO7J5CoSUl2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "scrolled": true,
        "id": "uakGThWv7el-"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_indices, test_indices = split_sample_data_fast(n_samples=480_000, seed=42, n_sub_samples=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_vqvae(train_indices, test_indices)"
      ],
      "metadata": {
        "id": "P0YhrJsCLu_z",
        "outputId": "a46cdec4-82d3-459e-8b0a-7e67fc2d4bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5701dd9b1815>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_vqvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-1e17ba237c49>\u001b[0m in \u001b[0;36mtrain_vqvae\u001b[0;34m(train_indices, test_indices, path)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    idxs = torch.randint(0, len(test), (100, )) # Randomly sample indices for reconstruction.\n",
        "    ims = torch.cat([test[idx][0][None, :] for idx in idxs]).float() # Create a batch of images from the test set using sampled indices.\n",
        "    ims = ims.to(device)\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    generated_im, _ = model(ims) # Generate reconstructed images.\n",
        "    ims = (ims+1)/2\n",
        "    generated_im = (generated_im + 1) / 2  # Normalize to [0, 1] for visualization\n",
        "\n",
        "    ## Transform to original value to retrieve colorized data\n",
        "    ims = ims * 255.0  # Scale to [0, 255]\n",
        "    generated_im = generated_im * 255.0  # Scale to [0, 255]\n",
        "    ims = ims.cpu().numpy().astype(np.uint8)\n",
        "    generated_im = generated_im.detach().cpu().numpy().astype(np.uint8)  # Detach before converting to NumPy\n",
        "\n",
        "    # combined_images = torch.hstack([ims, generated_im])\n",
        "    combined_images = np.concatenate((ims, generated_im), axis=3)\n",
        "\n",
        "    # Rearrange to a grid\n",
        "    combined_images = torch.tensor(combined_images).permute(0, 1, 2, 3)  # Change to [batch, channels, height, width]\n",
        "\n",
        "    grid = torchvision.utils.make_grid(combined_images, nrow=10, padding=2)\n",
        "\n",
        "    img = torchvision.transforms.ToPILImage()(grid)\n",
        "    #img.save('reconstruction.png')\n",
        "    im.show()\n",
        "\n",
        "    print('Done Reconstruction ...')\n"
      ],
      "metadata": {
        "id": "7LM1VJegAULv",
        "outputId": "68423342-beca-47d5-978c-141777146768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-37b457fed272>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Randomly sample indices for reconstruction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create a batch of images from the test set using sampled indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}