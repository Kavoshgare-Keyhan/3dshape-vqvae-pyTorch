{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, h5py, os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['images', 'labels']>\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = h5py.File('/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'r')\n",
    "print(dataset.keys())\n",
    "images = dataset['images']  # array shape [480000,64,64,3], uint8 in range(256)\n",
    "labels = dataset['labels']  # array shape [480000,6], float64\n",
    "image_shape = images.shape[1:]  # [64,64,3]\n",
    "label_shape = labels.shape[1:]  # [6]\n",
    "n_samples = labels.shape[0]  # 10*10*10*8*4*15=480000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_LATENT_K = 20                 # Number of codebook entries\n",
    "NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "BETA = 1.0                        # Weight for the commitment loss\n",
    "\n",
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "SIZE = None                       # Spatial size of latent embedding\n",
    "                                  # will be set dynamically in `build_vqvae\n",
    "\n",
    "VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
    "VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
    "VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
    "\n",
    "PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
    "PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, data_path, indices, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        assert os.path.exists(self.data_path), \"images path {} does not exist\".format(self.data_path)\n",
    "        self.data = h5py.File(self.data_path, 'r')\n",
    "        self.images = self.data['images'][indices]  # array shape [480000,64,64,3], uint8 in range(256)\n",
    "        self.labels = self.data['labels'][indices]  # array shape [480000,6], float64\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Normalize the image to range [-1, 1]\n",
    "        image = 2*(image.astype(np.float32)/255.0) - 1\n",
    "        # Transpose image to fit PyTorch's [C, H, W] format\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        \n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #super(VQVAE, self) is a python2 style for running super and call __init__() method of the parent class. In python3 we can use this new typping style.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 4, stride=2, padding=1), # inp_channels=1, out_channels=16, kernel_size (size of receptive field)=4 => width & height output = 14. We have also 16 Weight matrices that result in having an output of shape [14,14,16]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 4, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.pre_quant_conv = nn.Conv2d(4, 2, kernel_size=1) # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.embedding = nn.Embedding(num_embeddings=3, embedding_dim=2) # Three codebook vectors of size two - For more info regarding Embedding documentation: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.post_quant_conv = nn.Conv2d(2, 4, kernel_size=1)\n",
    "        \n",
    "        # Commitment Loss Beta\n",
    "        self.beta = 0.2\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4, 16, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # B, C, H, W\n",
    "        encoded_output = self.encoder(x)\n",
    "        quant_input = self.pre_quant_conv(encoded_output)\n",
    "        \n",
    "        ## Quantization\n",
    "        B, C, H, W = quant_input.shape\n",
    "        quant_input = quant_input.permute(0, 2, 3, 1)\n",
    "        quant_input = quant_input.reshape((quant_input.size(0), -1, quant_input.size(-1)))\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat((quant_input.size(0), 1, 1)))\n",
    "        \n",
    "        # Find index of nearest embedding\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "        \n",
    "        # Select the embedding weights\n",
    "        quant_out = torch.index_select(self.embedding.weight, 0, min_encoding_indices.view(-1))\n",
    "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
    "        \n",
    "        # Compute losses\n",
    "        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2) # commitment loss pushes z_x toward embedding space and updates encoded space. It ensures that it does not grow faster than embedding space\n",
    "        codebook_loss = torch.mean((quant_out - quant_input.detach())**2) # codebook loss pushes e_i toward z_x and update embedding space\n",
    "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
    "        \n",
    "        # Ensure straight through gradient\n",
    "        quant_out = quant_input + (quant_out - quant_input).detach()\n",
    "        \n",
    "        # Reshaping back to original input shape\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
    "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n",
    "        \n",
    "        \n",
    "        ## Decoder part\n",
    "        decoder_input = self.post_quant_conv(quant_out)\n",
    "        output = self.decoder(decoder_input)\n",
    "        return output, quantize_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sample_data(n_samples, seed=42, n_sub_samples=None):\n",
    "    np.random.seed(seed)\n",
    "    if n_sub_samples:\n",
    "        # Generate a list of n_sub_samples random integers between 0 and n_samples-1\n",
    "        indices = np.random.randint(0, n_samples, size=n_sub_samples)\n",
    "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
    "        n_test = int(len(indices)*0.2)\n",
    "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
    "        # Calculate the train indices (remaining 80% of the list)\n",
    "        train_indices = [i for i in indices if i not in test_indices]\n",
    "    else:\n",
    "        # Generate an array between 0 and n_samples-1\n",
    "        indices = np.arange(n_sample)\n",
    "        # Calculate the number of test indices (20% of the list) and extract the test indices\n",
    "        n_test = int(len(indices)*0.2)\n",
    "        test_indices = np.random.choice(indices, size=n_test, replace=False)\n",
    "        # Calculate the train indices (remaining 80% of the list)\n",
    "        train_indices = [i for i in indices if i not in test_indices]\n",
    "    return train_idices, test_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vqvae(train_indices, test_indices, path='/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5'):\n",
    "    train = LoadData(data_path=path, indices=train_indices)\n",
    "    test = LoadData(data_path=path, indices=test_indices)\n",
    "    mnist_loader = DataLoader(mnist, batch_size=64, shuffle=True, num_workers=4)\n",
    "    \n",
    "    model = VQVAE().to(device)\n",
    "    \n",
    "    num_epochs = 20\n",
    "    optimizer = Adam(model.parameters(), lr=1E-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for im, label in tqdm(mnist_loader):\n",
    "            im = im.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, quantize_loss = model(im)\n",
    "            \n",
    "            recon_loss = criterion(out, im)\n",
    "            loss = recon_loss + quantize_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Finished epoch {}'.format(epoch_idx+1))\n",
    "    print('Done Training...')\n",
    "    \n",
    "    # Reconstruction part\n",
    "    \n",
    "    idxs = torch.randint(0, len(mnist_test), (100, ))\n",
    "    ims = torch.cat([mnist_test[idx][0][None, :] for idx in idxs]).float()\n",
    "    ims = ims.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    generated_im, _ = model(ims)\n",
    "    ims = (ims+1)/2\n",
    "    generated_im = 1 - (generated_im+1)/2\n",
    "    out = torch.hstack([ims, generated_im])\n",
    "    output = rearrange(out, 'b c h w -> b () h (c w)')\n",
    "    grid = torchvision.utils.make_grid(output.detach().cpu(), nrow=10)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    img.save('reconstruction.png')\n",
    "    \n",
    "    print('Done Reconstruction ...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
