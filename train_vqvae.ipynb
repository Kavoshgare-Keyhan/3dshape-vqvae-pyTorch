{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ZahraFayyaz/3dshape-vqvae-pyTorch/blob/main/3dshape_vqvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Uqt-IlG47el4"},"source":["# Import modules"]},{"cell_type":"code","source":["!pip install einops==0.8.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GH0ffwQOpumu","executionInfo":{"status":"ok","timestamp":1721584329993,"user_tz":-120,"elapsed":5732,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}},"outputId":"1898bf79-919b-40c4-e1bd-1a3871070cde"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops==0.8.0 in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"]}]},{"cell_type":"code","source":["import sys, os, yaml\n","\n","# connect to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Enter the foldername where all modules are stored together\n","FOLDERNAME = 'INI - Generative Episodic Memory/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Change the working/current directory\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjbJ19kth4El","executionInfo":{"status":"ok","timestamp":1721584333412,"user_tz":-120,"elapsed":2112,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}},"outputId":"c37de7dd-9700-4b87-9500-d9c9114439e4"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/INI - Generative Episodic Memory\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Cmxk1jIK7el6","executionInfo":{"status":"ok","timestamp":1721584335796,"user_tz":-120,"elapsed":256,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from data_loader import load_data\n","from encoder import get_encoder\n","from quantizer import get_quantizer # returns embedding space automatically by calling quantizer.parameters(), q(z|x) as quant output, and z_q by calling quantize_indices method\n","from decoder import get_decoder\n","from vqvae import get_model"]},{"cell_type":"markdown","metadata":{"id":"6gRdbAotCqLw"},"source":["## Modify PyTorch Configuration"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"VSTPeJ38CqLx","executionInfo":{"status":"ok","timestamp":1721584338498,"user_tz":-120,"elapsed":3,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"}}},"outputs":[],"source":["# Enable CuDNN benchmark mode for faster runtime optimizations\n","torch.backends.cudnn.benchmark = True\n","\n","# Ensure deterministic behavior in CuDNN operations\n","torch.backends.cudnn.deterministic = True\n","\n","# Optionally disable CuDNN's tensor core usage if encountering issues\n","torch.backends.cudnn.enabled = True"]},{"cell_type":"markdown","metadata":{"id":"movEMY4M7el8"},"source":["# Load Data"]},{"cell_type":"code","source":["import argparse\n","import random\n","# import shutil\n","import cv2\n","import torchvision\n","import numpy as np\n","# from tqdm import tqdm\n","# from vqvae import get_model\n","from torch.optim import Adam\n","from torchvision.utils import make_grid\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train_for_one_epoch(epoch_idx, model, data_loader, optimizer, crtierion, config):\n","    r\"\"\"\n","    Method to run the training for one epoch.\n","    :param epoch_idx: iteration number of current epoch\n","    :param model: VQVAE model\n","    :param mnist_loader: Data loder for mnist\n","    :param optimizer: optimzier to be used taken from config\n","    :param crtierion: For computing the loss\n","    :param config: configuration for the current run\n","    :return:\n","    \"\"\"\n","    recon_losses = []\n","    codebook_losses = []\n","    commitment_losses = []\n","    losses = []\n","    # We ignore the label for VQVAE\n","    count = 0\n","    for im, _ in data_loader:\n","        im = im.float().to(device)\n","        optimizer.zero_grad()\n","        model_output = model(im)\n","        output = model_output['generated_image']\n","        quantize_losses = model_output['quantized_losses']\n","        # z_q = model_output['quantized_output']\n","        # indices = model_output['quantized_indices']\n","\n","        # if config['train_params']['save_training_image']:\n","        #     cv2.imwrite('input.jpeg', (255 * (im.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (im.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n","        #     cv2.imwrite('output.jpeg', (255 * (output.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (output.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n","\n","        recon_loss = crtierion(output, im)\n","        loss = (config['train_params']['reconstruction_loss_weight']*recon_loss +\n","                config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'] +\n","                config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'])\n","        recon_losses.append(recon_loss.item())\n","        codebook_losses.append(config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item())\n","        commitment_losses.append(quantize_losses['commitment_loss'].item())\n","        losses.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","    print('Finished epoch: {} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n","          format(epoch_idx + 1,\n","                 np.mean(recon_losses),\n","                 np.mean(codebook_losses),\n","                 np.mean(commitment_losses)))\n","    return np.mean(losses)\n","\n","\n","def train(config_path, sample=None):\n","    ######## Read the config file #######\n","    with open(config_path, 'r') as file:\n","        try:\n","            config = yaml.safe_load(file)\n","        except yaml.YAMLError as exc:\n","            print(exc)\n","    print(config)\n","    #######################################\n","\n","    ######## Set the desired seed value #######\n","    seed = config['train_params']['seed']\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","        # print(args.seed)\n","    #######################################\n","\n","    # Create the model and dataset\n","    model = get_model(config).to(device)\n","    data_loader = load_data(config['train_params']['path'],sample_data=sample)\n","    num_epochs = config['train_params']['epochs']\n","    optimizer = Adam(model.parameters(), lr=config['train_params']['lr'])\n","    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)\n","    criterion = {\n","        'l1': torch.nn.L1Loss(),\n","        'l2': torch.nn.MSELoss()\n","    }.get(config['train_params']['crit'])\n","\n","    # Create output directories\n","    if not os.path.exists(config['train_params']['task_name']):\n","        os.mkdir(config['train_params']['task_name'])\n","    if not os.path.exists(os.path.join(config['train_params']['task_name'],\n","                                       config['train_params']['output_train_dir'])):\n","        os.mkdir(os.path.join(config['train_params']['task_name'],\n","                              config['train_params']['output_train_dir']))\n","\n","    # Load checkpoint if found\n","    if os.path.exists(os.path.join(config['train_params']['task_name'],\n","                                                        config['train_params']['ckpt_name'])):\n","        print('Loading checkpoint')\n","        model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n","                                                      config['train_params']['ckpt_name']), map_location=device))\n","    best_loss = np.inf\n","\n","    for epoch_idx in range(num_epochs):\n","        mean_loss = train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config)\n","        scheduler.step(mean_loss)\n","        # Simply update checkpoint if found better version\n","        if mean_loss < best_loss:\n","            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n","            torch.save(model.state_dict(), os.path.join(config['train_params']['task_name'],\n","                                                        config['train_params']['ckpt_name']))\n","            best_loss = mean_loss\n","        else:\n","            print('No Loss Improvement')\n","\n","# parser = argparse.ArgumentParser(description='Arguments for vq vae training')\n","# parser.add_argument('--config', dest='config_path',\n","#                     default='/content/drive/My Drive/INI - Generative Episodic Memory/hyperparameters.yaml', type=str)\n","# args = parser.parse_args()\n","train(config_path='hyperparameters.yaml')\n","# kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n","# for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n","#     print(f\"Fold {fold + 1}\")\n","\n","#     # Create data samplers\n","#     train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","#     val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","#     # Create data loaders\n","#     train_loader = load_data(config['train_params']['path'],sample_data=train_sampler)\n","#     val_loader = load_data(config['train_params']['path'],sample_data=val_sampler)"],"metadata":{"id":"-mcnDTVS45Ih","colab":{"base_uri":"https://localhost:8080/"},"outputId":"60a08545-3d72-477b-f470-c04cb5e95768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'leaky', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 4, 4, 4], 'transpose_kernel_strides': [1, 2, 1, 1], 'transpose_activation_fn': 'leaky', 'latent_dim': 72, 'codebook_size': 10}, 'train_params': {'task_name': 'vqvae_latent_72_codebook_10_nnLayers_4', 'batch_size': 72, 'epochs': 10, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'ckpt_name': 'best_vqvae_latent_72_codebook_10.pth', 'seed': 42, 'save_training_image': True, 'path': '/content/drive/MyDrive/Data/3dshapes.h5', 'output_train_dir': 'output'}}\n","{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'leaky', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 4, 4, 4], 'transpose_kernel_strides': [1, 2, 1, 1], 'transpose_activation_fn': 'leaky', 'latent_dim': 72, 'codebook_size': 10}, 'train_params': {'task_name': 'vqvae_latent_72_codebook_10_nnLayers_4', 'batch_size': 72, 'epochs': 10, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'ckpt_name': 'best_vqvae_latent_72_codebook_10.pth', 'seed': 42, 'save_training_image': True, 'path': '/content/drive/MyDrive/Data/3dshapes.h5', 'output_train_dir': 'output'}}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}