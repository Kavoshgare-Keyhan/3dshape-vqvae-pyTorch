{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqt-IlG47el4"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymNc55c0x_vo",
        "outputId": "6b9c876a-fa96-47f0-be6b-faa93ef2c48b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install einops\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjbJ19kth4El",
        "outputId": "3c9bf06f-77ab-4d04-a389-f0849b4629f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/INI - Generative Episodic Memory\n"
          ]
        }
      ],
      "source": [
        "import sys, os, yaml\n",
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Enter the foldername where all modules are stored together\n",
        "FOLDERNAME = 'INI - Generative Episodic Memory/'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Change the working/current directory\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "idMQSyEjxh6_"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, random, csv, h5py, os, yaml, optuna\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from data_loader import Shapes3DDataset, custom_collate_fn #, load_data, load_test\n",
        "from vqvae import get_model\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "# from torchvision.utils import make_grid\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import SubsetRandomSampler, Subset, DataLoader\n",
        "from sklearn.model_selection import KFold, train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-mcnDTVS45Ih"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config):\n",
        "    r\"\"\"\n",
        "    Method to run the training for one epoch.\n",
        "    :param epoch_idx: iteration number of current epoch\n",
        "    :param model: VQVAE model\n",
        "    :param data_loader: Data loder for 3dshapes\n",
        "    :param optimizer: optimzier to be used taken from config\n",
        "    :param criterion: For computing the loss\n",
        "    :param config: configuration for the current run\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    recon_losses = []\n",
        "    codebook_losses = []\n",
        "    commitment_losses = []\n",
        "    losses = []\n",
        "\n",
        "    for im, label in tqdm(data_loader, desc='Training', leave=False): # Ignore the label in DataLoader\n",
        "        im = im.float().to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model_output = model(im)\n",
        "        output = model_output['generated_image']\n",
        "        quantize_losses = model_output['quantized_losses']\n",
        "\n",
        "        # if config['train_params']['save_training_image']:\n",
        "        #     cv2.imwrite('input.jpeg', (255 * (im.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (im.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n",
        "        #     cv2.imwrite('output.jpeg', (255 * (output.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (output.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n",
        "\n",
        "        recon_loss = criterion(output, im)\n",
        "        loss = (config['train_params']['reconstruction_loss_weight']*recon_loss +\n",
        "                config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'] +\n",
        "                config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'])\n",
        "        recon_losses.append(recon_loss.item())\n",
        "        codebook_losses.append(config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item())\n",
        "        commitment_losses.append(quantize_losses['commitment_loss'].item())\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Finished epoch: {} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n",
        "          format(epoch_idx + 1,\n",
        "                 np.mean(recon_losses),\n",
        "                 np.mean(codebook_losses),\n",
        "                 np.mean(commitment_losses)))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def train(config, data_loader, save_option=True): # learning_rate, sample=None\n",
        "    ######## Set the desired seed value #######\n",
        "    seed = config['train_params']['seed']\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        # print(args.seed)\n",
        "    #######################################\n",
        "\n",
        "    # Create the model and dataset\n",
        "    model = get_model(config).to(device)\n",
        "    # if data_loader is None:\n",
        "    #     if sample: data_loader = load_data(config['train_params']['path'],sample_data=sample, shuffle_data=False, batch_size=batch_size)\n",
        "    #     else: data_loader = load_data(config['train_params']['path'])\n",
        "    num_epochs = config['train_params']['epochs']\n",
        "    optimizer = Adam(model.parameters(), lr=config['train_params']['lr'])\n",
        "    # scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)\n",
        "    criterion = {\n",
        "        'l1': torch.nn.L1Loss(),\n",
        "        'l2': torch.nn.MSELoss()\n",
        "    }.get(config['train_params']['crit'])\n",
        "\n",
        "    # Create output directories\n",
        "    if not os.path.exists(config['train_params']['output_dir']):\n",
        "        os.mkdir(config['train_params']['output_dir'])\n",
        "\n",
        "    # Train the model\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        mean_loss = train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config)\n",
        "        # scheduler.step(mean_loss)\n",
        "        # Simply update checkpoint if found better version\n",
        "        if mean_loss < best_loss:\n",
        "            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n",
        "            best_loss = mean_loss\n",
        "        else:\n",
        "            print('No Loss Improvement')\n",
        "\n",
        "    # Save the final model and codebook indices\n",
        "    if save_option:\n",
        "        all_indices = []\n",
        "        with torch.no_grad():\n",
        "            for im, label in tqdm(data_loader, desc='Recall trained model to save codebook indices', leave=False): # Ignore the label in DataLoader\n",
        "                im = im.float().to(device)\n",
        "                model_output = model(im)\n",
        "                indices = model_output['quantized_indices']\n",
        "                all_indices.append(indices.cpu())\n",
        "\n",
        "        # Concatenate all indices into a single tensor and save it\n",
        "        indices_tensor = torch.cat(all_indices, dim=0)\n",
        "        torch.save(indices_tensor, os.path.join(config['train_params']['output_dir'], config['train_params']['indices_tensor']))\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(config['train_params']['output_dir'], config['train_params']['model_name']))\n",
        "\n",
        "    # return best_loss\n",
        "\n",
        "def validate(config, data_loader): #sample, batch_size\n",
        "    ######## Load saved model and assign it to a similar structure ########\n",
        "    state_dict = torch.load(os.path.join(config['train_params']['output_dir'], config['train_params']['model_name']))\n",
        "    model = get_model(config).to(device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    #######################################\n",
        "\n",
        "    # Validate the model with validation set\n",
        "    # val_loader = load_data(config['train_params']['path'],sample_data=sample, shuffle_data=False, batch_size=batch_size)\n",
        "    criterion = {\n",
        "        'l1': torch.nn.L1Loss(),\n",
        "        'l2': torch.nn.MSELoss()\n",
        "    }.get(config['train_params']['crit'])\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_recon_loss = 0\n",
        "        total_codebook_loss = 0\n",
        "        total_commitment_loss = 0\n",
        "        total_loss = 0\n",
        "        for im, label in data_loader:\n",
        "            im = im.float().to(device)\n",
        "            model_output = model(im)\n",
        "            output = model_output['generated_image']\n",
        "\n",
        "            recon_loss = criterion(output, im)\n",
        "            quantize_losses = model_output['quantized_losses']\n",
        "\n",
        "            total_recon_loss += recon_loss.item()\n",
        "            total_codebook_loss += quantize_losses['codebook_loss'].item()\n",
        "            total_commitment_loss += quantize_losses['commitment_loss'].item()\n",
        "            total_loss += (config['train_params']['reconstruction_loss_weight']*recon_loss.item() +\n",
        "                    config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item() +\n",
        "                    config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'].item())\n",
        "\n",
        "\n",
        "        avg_recon_loss = np.mean(total_recon_loss) # / len(val_loader)\n",
        "        avg_codebook_loss = np.mean(total_codebook_loss) # / len(val_loader)\n",
        "        avg_commitment_loss = np.mean(total_commitment_loss) # / len(val_loader)\n",
        "        avg_loss = np.mean(total_loss) # / len(val_loader)\n",
        "        print('Total Validation Loss : {:.4f} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n",
        "              format(avg_loss, avg_recon_loss, avg_codebook_loss, avg_commitment_loss))\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "def assess_test(config, sample):\n",
        "    ######## Load saved model and assign it to a similar structure ########\n",
        "    state_dict = torch.load(os.path.join(config['train_params']['output_dir'], config['train_params']['model_name']))\n",
        "    model = get_model(config).to(device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    #######################################\n",
        "\n",
        "    # Validate the model with validation set\n",
        "    test_loader = load_data(config['train_params']['path'],sample_data=sample, shuffle_data=False)\n",
        "    criterion = {\n",
        "        'l1': torch.nn.L1Loss(),\n",
        "        'l2': torch.nn.MSELoss()\n",
        "    }.get(config['train_params']['crit'])\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_recon_loss = 0\n",
        "        total_codebook_loss = 0\n",
        "        total_commitment_loss = 0\n",
        "        total_loss = 0\n",
        "        for im, label in test_loader:\n",
        "            im = im.float().to(device)\n",
        "            model_output = model(im)\n",
        "            output = model_output['generated_image']\n",
        "\n",
        "            recon_loss = criterion(output, im)\n",
        "            quantize_losses = model_output['quantized_losses']\n",
        "\n",
        "            total_recon_loss += recon_loss.item()\n",
        "            total_codebook_loss += quantize_losses['codebook_loss'].item()\n",
        "            total_commitment_loss += quantize_losses['commitment_loss'].item()\n",
        "            total_loss += (config['train_params']['reconstruction_loss_weight']*recon_loss.item() +\n",
        "                    config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item() +\n",
        "                    config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'].item())\n",
        "\n",
        "\n",
        "        avg_recon_loss = np.mean(total_recon_loss) # / len(test_loader)\n",
        "        avg_codebook_loss = np.mean(total_codebook_loss) # / len(test_loader)\n",
        "        avg_commitment_loss = np.mean(total_commitment_loss) # / len(test_loader)\n",
        "        avg_loss = np.mean(total_loss) # / len(test_loader)\n",
        "        print('Total Test Loss : {:.4f} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n",
        "              format(avg_loss, avg_recon_loss, avg_codebook_loss, avg_commitment_loss))\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def reconstruction(config, test_data): # I have to address test_data\n",
        "    idxs = torch.randint(0, len(test_dataset), (100, )) # Randomly sample indices for reconstruction.\n",
        "    ims = torch.cat([test_dataset[idx][0][None, :] for idx in idxs]).float() # Create a batch of images from the test set using sampled indices.\n",
        "    ims = ims.to(device)\n",
        "\n",
        "    model_output = model(ims) # Generate reconstructed images.\n",
        "    generated_ims = model_output['generated_image']\n",
        "    ims = (ims+1)/2\n",
        "    generated_ims = (generated_ims + 1) / 2  # Normalize to [0, 1] for visualization\n",
        "\n",
        "    ## Transform to original value to retrieve colorized data\n",
        "    ims = ims * 255.0  # Scale to [0, 255]\n",
        "    generated_ims = generated_ims * 255.0  # Scale to [0, 255]\n",
        "    ims = ims.cpu().numpy().astype(np.uint8)\n",
        "    generated_ims = generated_ims.detach().cpu().numpy().astype(np.uint8)  # Detach before converting to NumPy\n",
        "\n",
        "    # combined_images = torch.hstack([ims, generated_im])\n",
        "    combined_images = np.concatenate((ims, generated_ims), axis=3)\n",
        "\n",
        "    # Rearrange to a grid\n",
        "    combined_images = torch.tensor(combined_images).permute(0, 1, 2, 3)  # Change to [batch, channels, height, width]\n",
        "\n",
        "    grid = torchvision.utils.make_grid(combined_images, nrow=10, padding=2)\n",
        "\n",
        "    img = torchvision.transforms.ToPILImage()(grid)\n",
        "    #img.save('/content/drive/My Drive/INI - Generative Episodic Memory/reconstruction_{model_name}.png'.format(model_name=config['train_params']['model_name']))\n",
        "    img.save('reconstruction_{model_name}.png'.format(model_name=config['train_params']['model_name']))\n",
        "\n",
        "    #plot the rexonstructions in a large figure\n",
        "    plt.figure(figsize=(20, 10))\n",
        "    plt.imshow(img)\n",
        "\n",
        "    # embedding_weights = model.quantizer.embedding.weight.detach().cpu()\n",
        "    #torch.save(embedding_weights, 'learned_codebook.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vc4X-ZScuYO"
      },
      "source": [
        "> Best practice to do cross validation and determine hyperparameters\n",
        "\n",
        "\n",
        "If you want to train your model, fine-tune parameters, and do final evaluation, I recommend you to split your data into training|val|test.\n",
        "\n",
        "You fit your model using the training part, and then you check different parameter combinations on the val part. Finally, when you're sure which classifier/parameter obtains the best result on the val part, you evaluate on the test to get the final rest.\n",
        "\n",
        "Once you evaluate on the test part, you shouldn't change the parameters any more.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtyr55HVbtgD",
        "outputId": "fcd633c6-90d3-4509-c462-9f5d34c95256"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-08-22 14:06:42,932] A new study created in memory with name: no-name-75608e20-f604-45c6-977c-239ff9bcc54a\n",
            "<ipython-input-6-c3b2ce6a27a7>:32: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
            "<ipython-input-6-c3b2ce6a27a7>:37: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  commitment_cost = trial.suggest_uniform('commitment_cost', 0.1, 2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 202}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.0006758006869925689, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 1.2568476401822941, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/content/drive/MyDrive/Data/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|▊         | 408/4800 [2:07:31<8:10:16,  6.70s/it] "
          ]
        }
      ],
      "source": [
        " ######## Read the config file #######\n",
        "config_path='hyperparameters.yaml'\n",
        "with open(config_path, 'r') as file:\n",
        "    try:\n",
        "        config = yaml.safe_load(file)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "# print(config)\n",
        "\n",
        "def split_dataset(dataset, test_size=0.2):\n",
        "    dataset_size = len(dataset)\n",
        "    indices = np.arange(dataset_size)\n",
        "\n",
        "    train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=config['train_params']['seed'], shuffle=True)\n",
        "\n",
        "    # Create Subsets for the training and testing sets\n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=config['train_params']['seed'], shuffle=True)\n",
        "\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    # learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
        "    # config['train_params']['lr'] = learning_rate\n",
        "    batch_size = 256 #trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
        "    num_embeddings = trial.suggest_int('num_embeddings', 20, 256)\n",
        "    config['model_params']['codebook_size'] = num_embeddings\n",
        "    # commitment_cost = trial.suggest_uniform('commitment_cost', 0.1, 2.0)\n",
        "    # config['train_params']['commitment_loss_weight'] = commitment_cost\n",
        "\n",
        "    # Load the dataset and split into train and test\n",
        "    dataset = Shapes3DDataset(path=config['train_params']['path'])\n",
        "    train_dataset, test_dataset = split_dataset(dataset)\n",
        "\n",
        "    # Initialize KFold\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=config['train_params']['seed'])\n",
        "\n",
        "    # train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # KFold Cross-Validation on the training set\n",
        "    for train_idx, val_idx in kf.split(np.arange(len(train_dataset))): #for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(train_dataset))))# The code between two hash can be good if we want to do some logging on each fold index\n",
        "        # Create samplers\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        # Create DataLoaders for the current fold\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=train_sampler, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn)\n",
        "        val_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=val_sampler, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "        # train and save model while returning loss_train\n",
        "        train(config, data_loader=train_loader) #sample=train_sampler, batch_size=batch_size\n",
        "        # train_losses.append(train_loss)\n",
        "\n",
        "        # validate model\n",
        "        val_loss = validate(config, data_loader=val_loader) #sample=val_sampler, batch_size=batch_size\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    # Calculate the average validation loss across folds\n",
        "    avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "    return avg_val_loss\n",
        "\n",
        "# Optuna study\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(f\"Best hyperparameters: {study.best_params}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Test model and reconstruct image\n",
        "# test_loss = reconstruction(config, test_images)\n",
        "\n",
        "# file_name = 'model_assessment.csv'\n",
        "# # Check if the file exists\n",
        "# if not os.path.isfile(file_name):\n",
        "#     # Create the file and write column names\n",
        "#     with open(file_name, mode='w', newline='') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow(['model_name', 'avg_train_loss', 'avg_val_loss', 'test_loss'])  # Write column names\n",
        "# else:\n",
        "#     # Append data to the existing file\n",
        "#     with open(file_name, mode='a', newline='') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow([config['train_params']['model_name'], sum(val_losses)/len(val_losses), sum(train_losses)/len(train_losses), test_loss])  # Write data\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
