{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqt-IlG47el4"
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymNc55c0x_vo",
    "outputId": "04721345-0a0c-4cf2-d4bc-cdb691653175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjbJ19kth4El",
    "outputId": "4f837992-4528-40df-e567-ac903af949a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/My Drive/INI - Generative Episodic Memory\n"
     ]
    }
   ],
   "source": [
    "import sys, os, yaml\n",
    "# connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Enter the foldername where all modules are stored together\n",
    "FOLDERNAME = 'INI - Generative Episodic Memory/'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Change the working/current directory\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "idMQSyEjxh6_"
   },
   "outputs": [],
   "source": [
    "import torch, torchvision, random, csv, h5py, os, yaml, optuna\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from data_loader import load_data, load_test\n",
    "from vqvae import get_model\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torchvision.utils import make_grid\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-mcnDTVS45Ih"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config):\n",
    "    r\"\"\"\n",
    "    Method to run the training for one epoch.\n",
    "    :param epoch_idx: iteration number of current epoch\n",
    "    :param model: VQVAE model\n",
    "    :param data_loader: Data loder for 3dshapes\n",
    "    :param optimizer: optimzier to be used taken from config\n",
    "    :param criterion: For computing the loss\n",
    "    :param config: configuration for the current run\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    recon_losses = []\n",
    "    codebook_losses = []\n",
    "    commitment_losses = []\n",
    "    losses = []\n",
    "\n",
    "    for im, label in tqdm(data_loader, desc='Training', leave=False): # Ignore the label in DataLoader\n",
    "        im = im.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(im)\n",
    "        output = model_output['generated_image']\n",
    "        quantize_losses = model_output['quantized_losses']\n",
    "\n",
    "        # if config['train_params']['save_training_image']:\n",
    "        #     cv2.imwrite('input.jpeg', (255 * (im.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (im.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n",
    "        #     cv2.imwrite('output.jpeg', (255 * (output.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (output.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n",
    "\n",
    "        recon_loss = criterion(output, im)\n",
    "        loss = (config['train_params']['reconstruction_loss_weight']*recon_loss +\n",
    "                config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'] +\n",
    "                config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'])\n",
    "        recon_losses.append(recon_loss.item())\n",
    "        codebook_losses.append(config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item())\n",
    "        commitment_losses.append(quantize_losses['commitment_loss'].item())\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Finished epoch: {} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n",
    "          format(epoch_idx + 1,\n",
    "                 np.mean(recon_losses),\n",
    "                 np.mean(codebook_losses),\n",
    "                 np.mean(commitment_losses)))\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def train(config, batch_size, learning_rate, sample=None, save_option=True):\n",
    "    ######## Set the desired seed value #######\n",
    "    seed = config['train_params']['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # print(args.seed)\n",
    "    #######################################\n",
    "\n",
    "    # Create the model and dataset\n",
    "    model = get_model(config).to(device)\n",
    "    if sample: data_loader = load_data(config['train_params']['path'],sample_data=sample, shuffle_data=False, batch_size=batch_size)\n",
    "    else: data_loader = load_data(config['train_params']['path'])\n",
    "    num_epochs = config['train_params']['epochs']\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)\n",
    "    criterion = {\n",
    "        'l1': torch.nn.L1Loss(),\n",
    "        'l2': torch.nn.MSELoss()\n",
    "    }.get(config['train_params']['crit'])\n",
    "\n",
    "    # Create output directories\n",
    "    if not os.path.exists(config['train_params']['output_dir']):\n",
    "        os.mkdir(config['train_params']['output_dir'])\n",
    "\n",
    "    # Train the model\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        mean_loss = train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config)\n",
    "        # scheduler.step(mean_loss)\n",
    "        # Simply update checkpoint if found better version\n",
    "        if mean_loss < best_loss:\n",
    "            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n",
    "            best_loss = mean_loss\n",
    "        else:\n",
    "            print('No Loss Improvement')\n",
    "\n",
    "    # Save the final model and codebook indices\n",
    "    if save_option:\n",
    "        all_indices = []\n",
    "        with torch.no_grad():\n",
    "            for im, label in tqdm(data_loader, desc='Recall trained model to save codebook indices', leave=False): # Ignore the label in DataLoader\n",
    "                im = im.float().to(device)\n",
    "                model_output = model(im)\n",
    "                indices = model_output['quantized_indices']\n",
    "                all_indices.append(indices.cpu())\n",
    "\n",
    "        # Concatenate all indices into a single tensor and save it\n",
    "        indices_tensor = torch.cat(all_indices, dim=0)\n",
    "        torch.save(indices_tensor, os.path.join(config['train_params']['output_dir'], config['train_params']['indices_tensor']))\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(config['train_params']['output_dir'], config['train_params']['model_name']))\n",
    "\n",
    "    # return best_loss\n",
    "\n",
    "def validate(config, sample, batch_size):\n",
    "    ######## Load saved model and assign it to a similar structure ########\n",
    "    state_dict = torch.load(os.path.join(config['train_params']['output_dir'], config['train_params']['model_name']))\n",
    "    model = get_model(config).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    #######################################\n",
    "\n",
    "    # Validate the model with validation set\n",
    "    val_loader = load_data(config['train_params']['path'],sample_data=sample, shuffle_data=False, batch_size=batch_size)\n",
    "    criterion = {\n",
    "        'l1': torch.nn.L1Loss(),\n",
    "        'l2': torch.nn.MSELoss()\n",
    "    }.get(config['train_params']['crit'])\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_recon_loss = 0\n",
    "        total_codebook_loss = 0\n",
    "        total_commitment_loss = 0\n",
    "        total_loss = 0\n",
    "        for im, label in val_loader:\n",
    "            im = im.float().to(device)\n",
    "            model_output = model(im)\n",
    "            output = model_output['generated_image']\n",
    "\n",
    "            recon_loss = criterion(output, im)\n",
    "            quantize_losses = model_output['quantized_losses']\n",
    "\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_codebook_loss += quantize_losses['codebook_loss'].item()\n",
    "            total_commitment_loss += quantize_losses['commitment_loss'].item()\n",
    "            total_loss += (config['train_params']['reconstruction_loss_weight']*recon_loss.item() +\n",
    "                    config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item() +\n",
    "                    config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'].item())\n",
    "\n",
    "\n",
    "        avg_recon_loss = np.mean(total_recon_loss) # / len(val_loader)\n",
    "        avg_codebook_loss = np.mean(total_codebook_loss) # / len(val_loader)\n",
    "        avg_commitment_loss = np.mean(total_commitment_loss) # / len(val_loader)\n",
    "        avg_loss = np.mean(total_loss) # / len(val_loader)\n",
    "        print('Total Validation Loss : {:.4f} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n",
    "              format(avg_loss, avg_recon_loss, avg_codebook_loss, avg_commitment_loss))\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "def assess_test(config, sample):\n",
    "    ######## Load saved model and assign it to a similar structure ########\n",
    "    state_dict = torch.load(os.path.join(config['train_params']['output_dir'], config['train_params']['model_name']))\n",
    "    model = get_model(config).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    #######################################\n",
    "\n",
    "    # Validate the model with validation set\n",
    "    test_loader = load_data(config['train_params']['path'],sample_data=sample, shuffle_data=False)\n",
    "    criterion = {\n",
    "        'l1': torch.nn.L1Loss(),\n",
    "        'l2': torch.nn.MSELoss()\n",
    "    }.get(config['train_params']['crit'])\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_recon_loss = 0\n",
    "        total_codebook_loss = 0\n",
    "        total_commitment_loss = 0\n",
    "        total_loss = 0\n",
    "        for im, label in test_loader:\n",
    "            im = im.float().to(device)\n",
    "            model_output = model(im)\n",
    "            output = model_output['generated_image']\n",
    "\n",
    "            recon_loss = criterion(output, im)\n",
    "            quantize_losses = model_output['quantized_losses']\n",
    "\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_codebook_loss += quantize_losses['codebook_loss'].item()\n",
    "            total_commitment_loss += quantize_losses['commitment_loss'].item()\n",
    "            total_loss += (config['train_params']['reconstruction_loss_weight']*recon_loss.item() +\n",
    "                    config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item() +\n",
    "                    config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'].item())\n",
    "\n",
    "\n",
    "        avg_recon_loss = np.mean(total_recon_loss) # / len(test_loader)\n",
    "        avg_codebook_loss = np.mean(total_codebook_loss) # / len(test_loader)\n",
    "        avg_commitment_loss = np.mean(total_commitment_loss) # / len(test_loader)\n",
    "        avg_loss = np.mean(total_loss) # / len(test_loader)\n",
    "        print('Total Test Loss : {:.4f} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n",
    "              format(avg_loss, avg_recon_loss, avg_codebook_loss, avg_commitment_loss))\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def reconstruction(config, test_data): # I have to address test_data\n",
    "    idxs = torch.randint(0, len(test_dataset), (100, )) # Randomly sample indices for reconstruction.\n",
    "    ims = torch.cat([test_dataset[idx][0][None, :] for idx in idxs]).float() # Create a batch of images from the test set using sampled indices.\n",
    "    ims = ims.to(device)\n",
    "\n",
    "    model_output = model(ims) # Generate reconstructed images.\n",
    "    generated_ims = model_output['generated_image']\n",
    "    ims = (ims+1)/2\n",
    "    generated_ims = (generated_ims + 1) / 2  # Normalize to [0, 1] for visualization\n",
    "\n",
    "    ## Transform to original value to retrieve colorized data\n",
    "    ims = ims * 255.0  # Scale to [0, 255]\n",
    "    generated_ims = generated_ims * 255.0  # Scale to [0, 255]\n",
    "    ims = ims.cpu().numpy().astype(np.uint8)\n",
    "    generated_ims = generated_ims.detach().cpu().numpy().astype(np.uint8)  # Detach before converting to NumPy\n",
    "\n",
    "    # combined_images = torch.hstack([ims, generated_im])\n",
    "    combined_images = np.concatenate((ims, generated_ims), axis=3)\n",
    "\n",
    "    # Rearrange to a grid\n",
    "    combined_images = torch.tensor(combined_images).permute(0, 1, 2, 3)  # Change to [batch, channels, height, width]\n",
    "\n",
    "    grid = torchvision.utils.make_grid(combined_images, nrow=10, padding=2)\n",
    "\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "    #img.save('/content/drive/My Drive/INI - Generative Episodic Memory/reconstruction_{model_name}.png'.format(model_name=config['train_params']['model_name']))\n",
    "    img.save('reconstruction_{model_name}.png'.format(model_name=config['train_params']['model_name']))\n",
    "    \n",
    "    #plot the rexonstructions in a large figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(img)\n",
    "\n",
    "    # embedding_weights = model.quantizer.embedding.weight.detach().cpu()\n",
    "    #torch.save(embedding_weights, 'learned_codebook.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vc4X-ZScuYO"
   },
   "source": [
    "> Best practice to do cross validation and determine hyperparameters\n",
    "\n",
    "\n",
    "If you want to train your model, fine-tune parameters, and do final evaluation, I recommend you to split your data into training|val|test.\n",
    "\n",
    "You fit your model using the training part, and then you check different parameter combinations on the val part. Finally, when you're sure which classifier/parameter obtains the best result on the val part, you evaluate on the test to get the final rest.\n",
    "\n",
    "Once you evaluate on the test part, you shouldn't change the parameters any more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qtyr55HVbtgD",
    "outputId": "2aed88e9-d826-4d41-e6b0-66b6aa3013ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 16:25:15,015] A new study created in memory with name: no-name-21fbeb4f-c893-4915-9770-bc9752dd9779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                          | 0/50 [00:00<?, ?it/s]/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:952: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv_transpose2d(\n",
      "/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 1.0295 | Codebook Loss : 1.3228 | Commitment Loss : 1.3228\n",
      "Improved Loss to 2.6169 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.9853 | Codebook Loss : 1.3112 | Commitment Loss : 1.3112\n",
      "Improved Loss to 2.5587 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.9331 | Codebook Loss : 1.3000 | Commitment Loss : 1.3000\n",
      "Improved Loss to 2.4931 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.8741 | Codebook Loss : 1.2881 | Commitment Loss : 1.2881\n",
      "Improved Loss to 2.4198 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.8133 | Codebook Loss : 1.2752 | Commitment Loss : 1.2752\n",
      "Improved Loss to 2.3436 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss : 30.0228 | Recon Loss : 10.1786 | Codebook Loss : 16.5368 | Commitment Loss : 16.5368\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 1.0297 | Codebook Loss : 1.3231 | Commitment Loss : 1.3231\n",
      "Improved Loss to 2.6174 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.9854 | Codebook Loss : 1.3113 | Commitment Loss : 1.3113\n",
      "Improved Loss to 2.5590 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.9329 | Codebook Loss : 1.3001 | Commitment Loss : 1.3001\n",
      "Improved Loss to 2.4930 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.8733 | Codebook Loss : 1.2880 | Commitment Loss : 1.2880\n",
      "Improved Loss to 2.4189 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.8127 | Codebook Loss : 1.2751 | Commitment Loss : 1.2751\n",
      "Improved Loss to 2.3428 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss : 30.0103 | Recon Loss : 10.1763 | Codebook Loss : 16.5283 | Commitment Loss : 16.5283\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 1.0301 | Codebook Loss : 1.3228 | Commitment Loss : 1.3228\n",
      "Improved Loss to 2.6175 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.9856 | Codebook Loss : 1.3111 | Commitment Loss : 1.3111\n",
      "Improved Loss to 2.5590 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.9331 | Codebook Loss : 1.2998 | Commitment Loss : 1.2998\n",
      "Improved Loss to 2.4930 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.8736 | Codebook Loss : 1.2879 | Commitment Loss : 1.2879\n",
      "Improved Loss to 2.4191 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.8128 | Codebook Loss : 1.2751 | Commitment Loss : 1.2751\n",
      "Improved Loss to 2.3429 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss : 30.0535 | Recon Loss : 10.1863 | Codebook Loss : 16.5560 | Commitment Loss : 16.5560\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 1.0303 | Codebook Loss : 1.3229 | Commitment Loss : 1.3229\n",
      "Improved Loss to 2.6178 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.9859 | Codebook Loss : 1.3111 | Commitment Loss : 1.3111\n",
      "Improved Loss to 2.5593 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.9336 | Codebook Loss : 1.2998 | Commitment Loss : 1.2998\n",
      "Improved Loss to 2.4934 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.8745 | Codebook Loss : 1.2880 | Commitment Loss : 1.2880\n",
      "Improved Loss to 2.4201 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.8138 | Codebook Loss : 1.2752 | Commitment Loss : 1.2752\n",
      "Improved Loss to 2.3441 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss : 29.8855 | Recon Loss : 10.1697 | Codebook Loss : 16.4298 | Commitment Loss : 16.4298\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 1.0308 | Codebook Loss : 1.3230 | Commitment Loss : 1.3230\n",
      "Improved Loss to 2.6184 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.9864 | Codebook Loss : 1.3111 | Commitment Loss : 1.3111\n",
      "Improved Loss to 2.5597 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.9338 | Codebook Loss : 1.2996 | Commitment Loss : 1.2996\n",
      "Improved Loss to 2.4934 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.8742 | Codebook Loss : 1.2876 | Commitment Loss : 1.2876\n",
      "Improved Loss to 2.4193 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.8135 | Codebook Loss : 1.2748 | Commitment Loss : 1.2748\n",
      "Improved Loss to 2.3433 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 6}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-15 19:17:19,868] Trial 0 finished with value: 29.999846894741058 and parameters: {'batch_size': 64, 'num_embeddings': 6}. Best is trial 0 with value: 29.999846894741058.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss : 30.0271 | Recon Loss : 10.1602 | Codebook Loss : 16.5557 | Commitment Loss : 16.5557\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 5}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 0.8152 | Codebook Loss : 1.3576 | Commitment Loss : 1.3576\n",
      "Improved Loss to 2.4443 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.7894 | Codebook Loss : 1.3531 | Commitment Loss : 1.3531\n",
      "Improved Loss to 2.4131 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.7663 | Codebook Loss : 1.3482 | Commitment Loss : 1.3482\n",
      "Improved Loss to 2.3842 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.7457 | Codebook Loss : 1.3431 | Commitment Loss : 1.3431\n",
      "Improved Loss to 2.3574 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.7272 | Codebook Loss : 1.3376 | Commitment Loss : 1.3376\n",
      "Improved Loss to 2.3323 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 5}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n",
      "Total Validation Loss : 16.2161 | Recon Loss : 5.0226 | Codebook Loss : 9.3279 | Commitment Loss : 9.3279\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 5}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 1 | Recon Loss : 0.8152 | Codebook Loss : 1.3579 | Commitment Loss : 1.3579\n",
      "Improved Loss to 2.4447 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 2 | Recon Loss : 0.7895 | Codebook Loss : 1.3533 | Commitment Loss : 1.3533\n",
      "Improved Loss to 2.4134 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 3 | Recon Loss : 0.7664 | Codebook Loss : 1.3485 | Commitment Loss : 1.3485\n",
      "Improved Loss to 2.3846 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 4 | Recon Loss : 0.7457 | Codebook Loss : 1.3434 | Commitment Loss : 1.3434\n",
      "Improved Loss to 2.3578 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch: 5 | Recon Loss : 0.7273 | Codebook Loss : 1.3380 | Commitment Loss : 1.3380\n",
      "Improved Loss to 2.3329 .... Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 5}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Validation Loss : 16.2797 | Recon Loss : 5.0250 | Codebook Loss : 9.3789 | Commitment Loss : 9.3789\n",
      "{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'relu', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'relu', 'latent_dim': 72, 'codebook_size': 5}, 'train_params': {'batch_size': 72, 'epochs': 5, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'model_name': 'vqvae_nnL4_cdbk10_ld72.pth', 'indices_tensor': 'vqvae_nnL4_cdbk10_ld72.pt', 'seed': 42, 'save_training_image': True, 'path': '/home/mohsen/Desktop/Academia/RUB Research Projects/INI/data/3dshapes/3dshapes.h5', 'output_dir': 'vqvae_outputs'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-08-15 20:22:59,539] Trial 1 failed with parameters: {'batch_size': 128, 'num_embeddings': 5} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9257/1346518978.py\", line 38, in objective\n",
      "    train(config, sample=train_sampler, batch_size=batch_size, learning_rate=learning_rate)\n",
      "  File \"/tmp/ipykernel_9257/2638475025.py\", line 82, in train\n",
      "    mean_loss = train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_9257/2638475025.py\", line 19, in train_for_one_epoch\n",
      "    for im, label in tqdm(data_loader, desc='Training', leave=False): # Ignore the label in DataLoader\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/site-packages/tqdm/std.py\", line 1178, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1285, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1133, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/queue.py\", line 180, in get\n",
      "    self.not_empty.wait(remaining)\n",
      "  File \"/home/mohsen/anaconda3/lib/python3.11/threading.py\", line 331, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-08-15 20:22:59,542] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Optuna study\u001b[39;00m\n\u001b[1;32m     51\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Best hyperparameters\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m val_sampler \u001b[38;5;241m=\u001b[39m SubsetRandomSampler(val_index)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# train and save model while returning loss_train\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m train(config, sample\u001b[38;5;241m=\u001b[39mtrain_sampler, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# train_losses.append(train_loss)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# validate model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m validate(config, sample\u001b[38;5;241m=\u001b[39mval_sampler, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "Cell \u001b[0;32mIn[7], line 82\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, batch_size, learning_rate, sample, save_option)\u001b[0m\n\u001b[1;32m     79\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 82\u001b[0m     mean_loss \u001b[38;5;241m=\u001b[39m train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# scheduler.step(mean_loss)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Simply update checkpoint if found better version\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean_loss \u001b[38;5;241m<\u001b[39m best_loss:\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(epoch_idx, model, data_loader, optimizer, criterion, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m commitment_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im, label \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# Ignore the label in DataLoader\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " ######## Read the config file #######\n",
    "config_path='hyperparameters.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "# print(config)\n",
    "\n",
    "# Load data from the H5 file\n",
    "h5_file = config['train_params']['path']\n",
    "with h5py.File(h5_file, 'r') as f:\n",
    "    images = np.array(f['images'][:5_000,:])\n",
    "    labels = np.array(f['labels'][:5_000,:])\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=config['train_params']['seed'], shuffle=True)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256,512])\n",
    "    num_embeddings = trial.suggest_int('num_embeddings', 20, 512)\n",
    "    config['model_params']['codebook_size'] = num_embeddings\n",
    "    commitment_cost = trial.suggest_uniform('commitment_cost', 0.1, 2.0)\n",
    "    config['train_params']['commitment_loss_weight'] = commitment_cost\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=config['train_params']['seed'])\n",
    "    \n",
    "    # train_losses = []\n",
    "    val_losses = []\n",
    "    # Get indices for training and validation\n",
    "    for train_index, val_index in kf.split(train_images):\n",
    "        # Create samplers\n",
    "        train_sampler = SubsetRandomSampler(train_index)\n",
    "        val_sampler = SubsetRandomSampler(val_index)\n",
    "    \n",
    "        # train and save model while returning loss_train\n",
    "        train(config, sample=train_sampler, batch_size=batch_size, learning_rate=learning_rate)\n",
    "        # train_losses.append(train_loss)\n",
    "    \n",
    "        # validate model\n",
    "        val_loss = validate(config, sample=val_sampler, batch_size=batch_size)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    # Calculate the average validation loss across folds\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Test model and reconstruct image\n",
    "# test_loss = reconstruction(config, test_images)\n",
    "\n",
    "# file_name = 'model_assessment.csv'\n",
    "# # Check if the file exists\n",
    "# if not os.path.isfile(file_name):\n",
    "#     # Create the file and write column names\n",
    "#     with open(file_name, mode='w', newline='') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow(['model_name', 'avg_train_loss', 'avg_val_loss', 'test_loss'])  # Write column names\n",
    "# else:\n",
    "#     # Append data to the existing file\n",
    "#     with open(file_name, mode='a', newline='') as f:\n",
    "#         writer = csv.writer(f)\n",
    "#         writer.writerow([config['train_params']['model_name'], sum(val_losses)/len(val_losses), sum(train_losses)/len(train_losses), test_loss])  # Write data\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
