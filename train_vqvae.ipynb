{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/ZahraFayyaz/3dshape-vqvae-pyTorch/blob/main/3dshape_vqvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Uqt-IlG47el4"},"source":["# Import modules"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6565,"status":"ok","timestamp":1721673889798,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"},"user_tz":-120},"id":"GH0ffwQOpumu","outputId":"2775285e-8cbb-4675-aa10-077c4060834a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops==0.8.0\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}],"source":["!pip install einops==0.8.0"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63863,"status":"ok","timestamp":1721673839568,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"},"user_tz":-120},"id":"BjbJ19kth4El","outputId":"dc226597-4a98-42e1-fb7b-b7a2fcb874f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/INI - Generative Episodic Memory\n"]}],"source":["import sys, os, yaml\n","\n","# connect to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Enter the foldername where all modules are stored together\n","FOLDERNAME = 'INI - Generative Episodic Memory/'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Change the working/current directory\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","%cd /content/drive/My\\ Drive/$FOLDERNAME/"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4459,"status":"ok","timestamp":1721673897805,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"},"user_tz":-120},"id":"Cmxk1jIK7el6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from data_loader import load_data\n","from vqvae import get_model\n","# import argparse\n","import random\n","# import shutil\n","import cv2\n","import torchvision\n","import numpy as np\n","# from tqdm import tqdm\n","# from vqvae import get_model\n","from torch.optim import Adam\n","from torchvision.utils import make_grid\n","from torch.optim.lr_scheduler import ReduceLROnPlateau"]},{"cell_type":"markdown","metadata":{"id":"6gRdbAotCqLw"},"source":["## Modify PyTorch Configuration"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":551,"status":"ok","timestamp":1721673903395,"user":{"displayName":"Mohsen Abgharian","userId":"00588791625272270456"},"user_tz":-120},"id":"VSTPeJ38CqLx"},"outputs":[],"source":["# Enable CuDNN benchmark mode for faster runtime optimizations\n","torch.backends.cudnn.benchmark = True\n","\n","# Ensure deterministic behavior in CuDNN operations\n","torch.backends.cudnn.deterministic = True\n","\n","# Optionally disable CuDNN's tensor core usage if encountering issues\n","torch.backends.cudnn.enabled = True"]},{"cell_type":"markdown","metadata":{"id":"movEMY4M7el8"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mcnDTVS45Ih","outputId":"c4ac8fd4-a181-4a59-c02f-59da1b4f40b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'leaky', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'leaky', 'latent_dim': 72, 'codebook_size': 10}, 'train_params': {'task_name': 'vqvae_latent_72_codebook_10_nnLayers_4', 'batch_size': 72, 'epochs': 10, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'ckpt_name': 'best_vqvae_latent_72_codebook_10.pth', 'seed': 42, 'save_training_image': True, 'path': '/content/drive/MyDrive/Data/3dshapes.h5', 'output_train_dir': 'output'}}\n","{'model_params': {'in_channels': 3, 'convbn_blocks': 4, 'conv_kernel_size': [3, 3, 3, 2], 'conv_kernel_strides': [2, 2, 1, 1], 'convbn_channels': [3, 6, 12, 24, 72], 'conv_activation_fn': 'leaky', 'transpose_bn_blocks': 4, 'transposebn_channels': [72, 24, 12, 6, 3], 'transpose_kernel_size': [3, 3, 3, 2], 'transpose_kernel_strides': [2, 2, 1, 1], 'transpose_activation_fn': 'leaky', 'latent_dim': 72, 'codebook_size': 10}, 'train_params': {'task_name': 'vqvae_latent_72_codebook_10_nnLayers_4', 'batch_size': 72, 'epochs': 10, 'lr': 0.005, 'crit': 'l2', 'reconstruction_loss_weight': 1, 'codebook_loss_weight': 1, 'commitment_loss_weight': 0.2, 'ckpt_name': 'best_vqvae_latent_72_codebook_10.pth', 'seed': 42, 'save_training_image': True, 'path': '/content/drive/MyDrive/Data/3dshapes.h5', 'output_train_dir': 'output'}}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def train_for_one_epoch(epoch_idx, model, data_loader, optimizer, crtierion, config):\n","    r\"\"\"\n","    Method to run the training for one epoch.\n","    :param epoch_idx: iteration number of current epoch\n","    :param model: VQVAE model\n","    :param mnist_loader: Data loder for mnist\n","    :param optimizer: optimzier to be used taken from config\n","    :param crtierion: For computing the loss\n","    :param config: configuration for the current run\n","    :return:\n","    \"\"\"\n","    recon_losses = []\n","    codebook_losses = []\n","    commitment_losses = []\n","    losses = []\n","    # We ignore the label for VQVAE\n","    count = 0\n","    for im, _ in data_loader:\n","        im = im.float().to(device)\n","        optimizer.zero_grad()\n","        model_output = model(im)\n","        output = model_output['generated_image']\n","        quantize_losses = model_output['quantized_losses']\n","        # z_q = model_output['quantized_output']\n","        # indices = model_output['quantized_indices']\n","\n","        # if config['train_params']['save_training_image']:\n","        #     cv2.imwrite('input.jpeg', (255 * (im.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (im.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n","        #     cv2.imwrite('output.jpeg', (255 * (output.detach() + 1) / 2).cpu().permute(0, 1, 2, 3).numpy().astype(np.uint8)) #(255 * (output.detach() + 1) / 2).cpu().permute((0, 2, 3, 1)).numpy()[0]\n","\n","        recon_loss = crtierion(output, im)\n","        loss = (config['train_params']['reconstruction_loss_weight']*recon_loss +\n","                config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'] +\n","                config['train_params']['commitment_loss_weight']*quantize_losses['commitment_loss'])\n","        recon_losses.append(recon_loss.item())\n","        codebook_losses.append(config['train_params']['codebook_loss_weight']*quantize_losses['codebook_loss'].item())\n","        commitment_losses.append(quantize_losses['commitment_loss'].item())\n","        losses.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","    print('Finished epoch: {} | Recon Loss : {:.4f} | Codebook Loss : {:.4f} | Commitment Loss : {:.4f}'.\n","          format(epoch_idx + 1,\n","                 np.mean(recon_losses),\n","                 np.mean(codebook_losses),\n","                 np.mean(commitment_losses)))\n","    return np.mean(losses)\n","\n","\n","def train(config_path, sample=None):\n","    ######## Read the config file #######\n","    with open(config_path, 'r') as file:\n","        try:\n","            config = yaml.safe_load(file)\n","        except yaml.YAMLError as exc:\n","            print(exc)\n","    print(config)\n","    #######################################\n","\n","    ######## Set the desired seed value #######\n","    seed = config['train_params']['seed']\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","        # print(args.seed)\n","    #######################################\n","\n","    # Create the model and dataset\n","    model = get_model(config).to(device)\n","    data_loader = load_data(config['train_params']['path'],sample_data=sample)\n","    num_epochs = config['train_params']['epochs']\n","    optimizer = Adam(model.parameters(), lr=config['train_params']['lr'])\n","    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=1, verbose=True)\n","    criterion = {\n","        'l1': torch.nn.L1Loss(),\n","        'l2': torch.nn.MSELoss()\n","    }.get(config['train_params']['crit'])\n","\n","    # Create output directories\n","    if not os.path.exists(config['train_params']['task_name']):\n","        os.mkdir(config['train_params']['task_name'])\n","    if not os.path.exists(os.path.join(config['train_params']['task_name'],\n","                                       config['train_params']['output_train_dir'])):\n","        os.mkdir(os.path.join(config['train_params']['task_name'],\n","                              config['train_params']['output_train_dir']))\n","\n","    # Load checkpoint if found\n","    if os.path.exists(os.path.join(config['train_params']['task_name'],\n","                                                        config['train_params']['ckpt_name'])):\n","        print('Loading checkpoint')\n","        model.load_state_dict(torch.load(os.path.join(config['train_params']['task_name'],\n","                                                      config['train_params']['ckpt_name']), map_location=device))\n","    best_loss = np.inf\n","\n","    for epoch_idx in range(num_epochs):\n","        mean_loss = train_for_one_epoch(epoch_idx, model, data_loader, optimizer, criterion, config)\n","        scheduler.step(mean_loss)\n","        # Simply update checkpoint if found better version\n","        if mean_loss < best_loss:\n","            print('Improved Loss to {:.4f} .... Saving Model'.format(mean_loss))\n","            torch.save(model.state_dict(), os.path.join(config['train_params']['task_name'],\n","                                                        config['train_params']['ckpt_name']))\n","            best_loss = mean_loss\n","        else:\n","            print('No Loss Improvement')\n","\n","    return model\n","\n","\n","trained_model = train(config_path='hyperparameters.yaml')\n","# kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n","# for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n","#     print(f\"Fold {fold + 1}\")\n","\n","#     # Create data samplers\n","#     train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","#     val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","#     # Create data loaders\n","#     train_loader = load_data(config['train_params']['path'],sample_data=train_sampler)\n","#     val_loader = load_data(config['train_params']['path'],sample_data=val_sampler)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}